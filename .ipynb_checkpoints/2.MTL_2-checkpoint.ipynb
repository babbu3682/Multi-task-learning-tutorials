{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t85Nx2fQEF_s",
    "outputId": "09968e41-cfb5-47bd-8e6d-4a4a85cf2bbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 16 19:29:21 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:18:00.0 Off |                  Off |\n",
      "| 70%   89C    P2   246W / 300W |  11154MiB / 48685MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    Off  | 00000000:3B:00.0 Off |                  Off |\n",
      "|100%   85C    P2   119W / 300W |  45210MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    Off  | 00000000:86:00.0 Off |                  Off |\n",
      "| 58%   75C    P8    46W / 300W |      3MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000    Off  | 00000000:AF:00.0 Off |                  Off |\n",
      "| 68%   82C    P2   242W / 300W |  25408MiB / 48685MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "import cv2\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P: 75 (22.73%) N: 255 (77.27%) Total: 330\n",
    "P: 14 (20.00%) N: 56  (80.00%) Total: 70\n",
    "P: 21 (21.00%) N: 79  (79.00%) Total: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/sunggu/7.KOHI/Multi_task_learning_tutorials'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CN3SCwTVEF_v"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = './SSIM_seg/'\n",
    "\n",
    "x_train_list = glob.glob(os.path.join(DATA_DIR, 'train/*'))\n",
    "y_train_list = glob.glob(os.path.join(DATA_DIR, 'trainannot/*'))\n",
    "\n",
    "x_valid_list = glob.glob(os.path.join(DATA_DIR, 'val/*'))\n",
    "y_valid_list = glob.glob(os.path.join(DATA_DIR, 'valannot/*'))\n",
    "\n",
    "x_test_list  = glob.glob(os.path.join(DATA_DIR, 'test/*'))\n",
    "y_test_list  = glob.glob(os.path.join(DATA_DIR, 'testannot/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./SSIM_seg/train/1.2.276.0.7230010.3.1.4.8323329.300.1517875162.258081.png'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./SSIM_seg/val/1.2.276.0.7230010.3.1.4.8323329.1173.1517875166.626582.png'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "f0iPTFh_EF_w"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Dataset Class\n",
    "class Dataset(BaseDataset):    \n",
    "    def __init__(self, images_list, labels_list, transform):\n",
    "        self.images_list  = images_list\n",
    "        self.labels_list  = labels_list\n",
    "        self.transform    = transform\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # read data\n",
    "        image = cv2.imread(self.images_list[i], cv2.IMREAD_GRAYSCALE)\n",
    "        mask  = cv2.imread(self.labels_list[i], cv2.IMREAD_GRAYSCALE)\n",
    "        path  = self.images_list[i]\n",
    "        \n",
    "        mask  = np.expand_dims(mask, axis=0)\n",
    "            \n",
    "        # apply transform\n",
    "        sample = self.transform(image=image, mask=mask)\n",
    "        image, mask = sample['image'], sample['mask']        \n",
    "        \n",
    "        return image, mask, path\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     12,
     35
    ]
   },
   "outputs": [],
   "source": [
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import albumentations as albu\n",
    "\n",
    "\n",
    "def minmax_normalize(image, **kwargs):\n",
    "    if len(np.unique(image)) != 1:  # Sometimes it cause the nan inputs...\n",
    "        image = image.astype('float32')\n",
    "        image -= image.min()\n",
    "        image /= image.max() \n",
    "    return image\n",
    "\n",
    "\n",
    "train_transform = albu.Compose([\n",
    "    albu.HorizontalFlip(p=0.5),\n",
    "    albu.ShiftScaleRotate(scale_limit=0.10, shift_limit=0.10, rotate_limit=15, p=0.5),\n",
    "    albu.GaussNoise(p=0.2),\n",
    "    albu.OneOf(\n",
    "        [\n",
    "            albu.CLAHE(p=1),\n",
    "            albu.RandomBrightnessContrast(p=1),\n",
    "            albu.RandomGamma(p=1),\n",
    "        ],\n",
    "        p=0.3,\n",
    "    ),\n",
    "    albu.OneOf(\n",
    "        [\n",
    "            albu.Blur(blur_limit=3, p=1),\n",
    "            albu.MotionBlur(blur_limit=3, p=1),\n",
    "        ],\n",
    "        p=0.3,\n",
    "    ),\n",
    "    albu.Lambda(image=minmax_normalize, always_apply=True),\n",
    "    ToTensorV2(),    \n",
    "])\n",
    "\n",
    "valid_transform = albu.Compose([        \n",
    "    albu.Lambda(image=minmax_normalize, always_apply=True),\n",
    "    ToTensorV2(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset(images_list=x_train_list, labels_list=y_train_list, transform=train_transform)\n",
    "dataset_valid = Dataset(images_list=x_valid_list, labels_list=y_valid_list, transform=valid_transform)\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=10, num_workers=4, shuffle=True, pin_memory=True, drop_last=True)\n",
    "data_loader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=1,  num_workers=4, shuffle=True, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "cdMO4sJyEF_z",
    "outputId": "503b9008-e50a-41e5-bf09-ce4576d83d5f"
   },
   "outputs": [],
   "source": [
    "# # same image with different random transforms\n",
    "\n",
    "# batch = next(iter(train_loader))\n",
    "# x = batch['x'][0]\n",
    "# y_seg = batch['y_seg'][0]\n",
    "# y_cls = batch['y_cls'][0]\n",
    "\n",
    "# print(x.shape,y_seg.shape,y_cls.shape)\n",
    "# print(torch.unique(y_seg),y_cls)\n",
    "# visualize(image=x, mask=y_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Learnable Params: 18077613\n"
     ]
    }
   ],
   "source": [
    "from arch.smart_net import *\n",
    "from losses import MTL_Loss\n",
    "\n",
    "# Model\n",
    "model        = MTL_2_Net(encoder_name='resnet18').to('cuda')     \n",
    "\n",
    "# Loss\n",
    "criterion    = MTL_Loss(name='MTL_2')\n",
    "\n",
    "# Optimizer & LR Schedule   \n",
    "optimizer    = torch.optim.AdamW(params=model.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-4, amsgrad=False)\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Number of Learnable Params:', n_parameters)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/33]  eta: 0:00:28  lr: 0.000100  loss: 2.2062 (2.2062)  CLS_Loss: 0.8414 (0.8414)  SEG_Loss: 0.6883 (0.6883)  REC_Loss: 0.6766 (0.6766)  time: 0.8579  data: 0.6162  max mem: 3590\n",
      "Epoch: [0]  [10/33]  eta: 0:00:04  lr: 0.000100  loss: 1.6213 (1.7730)  CLS_Loss: 0.6420 (0.7967)  SEG_Loss: 0.5226 (0.4925)  REC_Loss: 0.4522 (0.4837)  time: 0.2134  data: 0.0561  max mem: 3804\n",
      "Epoch: [0]  [20/33]  eta: 0:00:02  lr: 0.000100  loss: 1.4848 (1.5501)  CLS_Loss: 0.6211 (0.7082)  SEG_Loss: 0.4752 (0.4557)  REC_Loss: 0.3191 (0.3862)  time: 0.1495  data: 0.0001  max mem: 3804\n",
      "Epoch: [0]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 1.1940 (1.4644)  CLS_Loss: 0.5642 (0.6850)  SEG_Loss: 0.3560 (0.4468)  REC_Loss: 0.2463 (0.3326)  time: 0.1496  data: 0.0002  max mem: 3804\n",
      "Epoch: [0]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 1.2248 (1.4599)  CLS_Loss: 0.6028 (0.6918)  SEG_Loss: 0.3560 (0.4444)  REC_Loss: 0.2341 (0.3237)  time: 0.1493  data: 0.0001  max mem: 3804\n",
      "Epoch: [0] Total time: 0:00:05 (0.1764 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 1.4598925, 'CLS_Loss': 0.691841, 'SEG_Loss': 0.4443652, 'REC_Loss': 0.3236863}\n",
      "Valid:  [ 0/70]  eta: 0:00:31  loss: 0.2530 (0.2530)  CLS_Loss: 0.0567 (0.0567)  SEG_Loss: 0.0000 (0.0000)  REC_Loss: 0.1963 (0.1963)  time: 0.4515  data: 0.4207  max mem: 3804\n",
      "Valid:  [10/70]  eta: 0:00:03  loss: 0.3415 (0.8327)  CLS_Loss: 0.0848 (0.5403)  SEG_Loss: 0.0000 (0.0530)  REC_Loss: 0.2468 (0.2394)  dice: 0.9017 (1.0140)  time: 0.0590  data: 0.0384  max mem: 3804\n",
      "Valid:  [20/70]  eta: 0:00:02  loss: 0.3371 (0.7571)  CLS_Loss: 0.0878 (0.4576)  SEG_Loss: 0.0000 (0.0605)  REC_Loss: 0.2339 (0.2390)  dice: 0.9017 (0.8082)  time: 0.0194  data: 0.0001  max mem: 3804\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.3363 (0.7766)  CLS_Loss: 0.0896 (0.4894)  SEG_Loss: 0.0000 (0.0507)  REC_Loss: 0.2317 (0.2365)  dice: 0.9646 (0.8804)  time: 0.0183  data: 0.0004  max mem: 3804\n",
      "Valid:  [40/70]  eta: 0:00:00  loss: 0.3376 (0.7818)  CLS_Loss: 0.0868 (0.5050)  SEG_Loss: 0.0000 (0.0450)  REC_Loss: 0.2204 (0.2318)  dice: 0.9646 (0.9591)  time: 0.0166  data: 0.0004  max mem: 3804\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.3203 (0.8011)  CLS_Loss: 0.0832 (0.5240)  SEG_Loss: 0.0000 (0.0445)  REC_Loss: 0.2270 (0.2327)  dice: 0.9646 (0.9889)  time: 0.0154  data: 0.0001  max mem: 3804\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.3211 (0.8088)  CLS_Loss: 0.0819 (0.5355)  SEG_Loss: 0.0000 (0.0408)  REC_Loss: 0.2293 (0.2326)  dice: 0.9646 (1.0415)  time: 0.0150  data: 0.0001  max mem: 3804\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.3361 (0.8607)  CLS_Loss: 0.0810 (0.5795)  SEG_Loss: 0.0000 (0.0474)  REC_Loss: 0.2418 (0.2339)  dice: 0.9488 (1.0228)  time: 0.0145  data: 0.0001  max mem: 3804\n",
      "Valid: Total time: 0:00:01 (0.0253 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.8607311, 'CLS_Loss': 0.5794817, 'SEG_Loss': 0.0473819, 'REC_Loss': 0.2338676, 'dice': 1.0228414, 'auc': 0.5076531, 'f1': 0.0, 'acc': 0.8, 'sen': 0.0, 'spe': 1.0}\n",
      "Epoch: [1]  [ 0/33]  eta: 0:00:18  lr: 0.000100  loss: 1.6533 (1.6533)  CLS_Loss: 0.5911 (0.5911)  SEG_Loss: 0.8879 (0.8879)  REC_Loss: 0.1744 (0.1744)  time: 0.5714  data: 0.4136  max mem: 4095\n",
      "Epoch: [1]  [10/33]  eta: 0:00:04  lr: 0.000100  loss: 1.2864 (1.2682)  CLS_Loss: 0.6117 (0.6868)  SEG_Loss: 0.4370 (0.4113)  REC_Loss: 0.1724 (0.1701)  time: 0.1885  data: 0.0377  max mem: 4095\n",
      "Epoch: [1]  [20/33]  eta: 0:00:02  lr: 0.000100  loss: 1.2349 (1.3076)  CLS_Loss: 0.6117 (0.7027)  SEG_Loss: 0.4112 (0.4456)  REC_Loss: 0.1574 (0.1593)  time: 0.1513  data: 0.0002  max mem: 4095\n",
      "Epoch: [1]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 1.2889 (1.3181)  CLS_Loss: 0.6679 (0.7104)  SEG_Loss: 0.4112 (0.4581)  REC_Loss: 0.1368 (0.1496)  time: 0.1535  data: 0.0002  max mem: 4095\n",
      "Epoch: [1]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 1.2100 (1.3107)  CLS_Loss: 0.6795 (0.7148)  SEG_Loss: 0.4050 (0.4479)  REC_Loss: 0.1349 (0.1480)  time: 0.1540  data: 0.0001  max mem: 4095\n",
      "Epoch: [1] Total time: 0:00:05 (0.1708 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 1.3106622, 'CLS_Loss': 0.7147501, 'SEG_Loss': 0.4479084, 'REC_Loss': 0.1480037}\n",
      "Valid:  [ 0/70]  eta: 0:00:35  loss: 0.2553 (0.2553)  CLS_Loss: 0.1540 (0.1540)  SEG_Loss: 0.0000 (0.0000)  REC_Loss: 0.1012 (0.1012)  time: 0.5120  data: 0.4847  max mem: 4095\n",
      "Valid:  [10/70]  eta: 0:00:03  loss: 0.2613 (0.5577)  CLS_Loss: 0.1350 (0.3921)  SEG_Loss: 0.0000 (0.0311)  REC_Loss: 0.1337 (0.1345)  dice: 0.9230 (1.1269)  time: 0.0613  data: 0.0441  max mem: 4095\n",
      "Valid:  [20/70]  eta: 0:00:01  loss: 0.2676 (0.5358)  CLS_Loss: 0.1350 (0.3984)  SEG_Loss: 0.0000 (0.0124)  REC_Loss: 0.1226 (0.1250)  dice: 1.3308 (1.3987)  time: 0.0157  data: 0.0001  max mem: 4095\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.2769 (0.7230)  CLS_Loss: 0.1710 (0.5466)  SEG_Loss: 0.0000 (0.0519)  REC_Loss: 0.1188 (0.1245)  dice: 0.9230 (1.0863)  time: 0.0154  data: 0.0001  max mem: 4095\n",
      "Valid:  [40/70]  eta: 0:00:00  loss: 0.2803 (0.7022)  CLS_Loss: 0.1671 (0.5313)  SEG_Loss: 0.0000 (0.0457)  REC_Loss: 0.1258 (0.1252)  dice: 1.2319 (1.1428)  time: 0.0152  data: 0.0001  max mem: 4095\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.2243 (0.7064)  CLS_Loss: 0.0900 (0.5434)  SEG_Loss: 0.0000 (0.0382)  REC_Loss: 0.1267 (0.1248)  dice: 1.3308 (1.2266)  time: 0.0151  data: 0.0001  max mem: 4095\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.2243 (0.7577)  CLS_Loss: 0.0930 (0.5947)  SEG_Loss: 0.0000 (0.0376)  REC_Loss: 0.1256 (0.1253)  dice: 1.3308 (1.2339)  time: 0.0152  data: 0.0001  max mem: 4095\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.2551 (0.7310)  CLS_Loss: 0.1178 (0.5676)  SEG_Loss: 0.0000 (0.0376)  REC_Loss: 0.1284 (0.1258)  dice: 1.2319 (1.2144)  time: 0.0147  data: 0.0001  max mem: 4095\n",
      "Valid: Total time: 0:00:01 (0.0246 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.7310168, 'CLS_Loss': 0.5676227, 'SEG_Loss': 0.0376427, 'REC_Loss': 0.1257513, 'dice': 1.2144337, 'auc': 0.5790816, 'f1': 0.0, 'acc': 0.7857143, 'sen': 0.0, 'spe': 0.9821429}\n",
      "Epoch: [2]  [ 0/33]  eta: 0:00:22  lr: 0.000100  loss: 1.0797 (1.0797)  CLS_Loss: 0.4848 (0.4848)  SEG_Loss: 0.4896 (0.4896)  REC_Loss: 0.1053 (0.1053)  time: 0.6895  data: 0.5372  max mem: 4345\n",
      "Epoch: [2]  [10/33]  eta: 0:00:04  lr: 0.000100  loss: 1.1079 (1.1779)  CLS_Loss: 0.6099 (0.6270)  SEG_Loss: 0.4896 (0.4350)  REC_Loss: 0.1123 (0.1159)  time: 0.2046  data: 0.0490  max mem: 4345\n",
      "Epoch: [2]  [20/33]  eta: 0:00:02  lr: 0.000100  loss: 1.1079 (1.1231)  CLS_Loss: 0.5647 (0.5951)  SEG_Loss: 0.3908 (0.4150)  REC_Loss: 0.1078 (0.1130)  time: 0.1573  data: 0.0002  max mem: 4345\n",
      "Epoch: [2]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.9625 (1.0745)  CLS_Loss: 0.5611 (0.6117)  SEG_Loss: 0.2937 (0.3519)  REC_Loss: 0.1074 (0.1109)  time: 0.1592  data: 0.0001  max mem: 4345\n",
      "Epoch: [2]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 1.0538 (1.0924)  CLS_Loss: 0.5006 (0.6150)  SEG_Loss: 0.3326 (0.3671)  REC_Loss: 0.1065 (0.1103)  time: 0.1594  data: 0.0001  max mem: 4345\n",
      "Epoch: [2] Total time: 0:00:05 (0.1796 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 1.0924241, 'CLS_Loss': 0.6149929, 'SEG_Loss': 0.367143, 'REC_Loss': 0.1102883}\n",
      "Valid:  [ 0/70]  eta: 0:00:32  loss: 0.1825 (0.1825)  CLS_Loss: 0.0688 (0.0688)  SEG_Loss: 0.0000 (0.0000)  REC_Loss: 0.1137 (0.1137)  time: 0.4591  data: 0.4347  max mem: 4345\n",
      "Valid:  [10/70]  eta: 0:00:03  loss: 0.5079 (0.8763)  CLS_Loss: 0.4349 (0.7868)  SEG_Loss: 0.0000 (-0.0034)  REC_Loss: 0.0934 (0.0929)  dice: 1.6861 (1.5461)  time: 0.0574  data: 0.0396  max mem: 4345\n",
      "Valid:  [20/70]  eta: 0:00:01  loss: 0.2878 (0.6051)  CLS_Loss: 0.2030 (0.5073)  SEG_Loss: 0.0000 (-0.0018)  REC_Loss: 0.1006 (0.0996)  dice: 1.6861 (1.5461)  time: 0.0171  data: 0.0001  max mem: 4345\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.2867 (0.5582)  CLS_Loss: 0.1872 (0.4650)  SEG_Loss: 0.0000 (-0.0044)  REC_Loss: 0.0964 (0.0976)  dice: 1.8197 (1.6008)  time: 0.0172  data: 0.0001  max mem: 4345\n",
      "Valid:  [40/70]  eta: 0:00:00  loss: 0.3295 (0.6488)  CLS_Loss: 0.2407 (0.5429)  SEG_Loss: 0.0000 (0.0095)  REC_Loss: 0.0936 (0.0964)  dice: 1.6232 (1.4709)  time: 0.0166  data: 0.0001  max mem: 4345\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.3495 (0.7537)  CLS_Loss: 0.2557 (0.6253)  SEG_Loss: 0.0000 (0.0305)  REC_Loss: 0.0986 (0.0979)  dice: 1.3389 (1.2996)  time: 0.0149  data: 0.0001  max mem: 4345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.3776 (0.7219)  CLS_Loss: 0.2825 (0.5945)  SEG_Loss: 0.0000 (0.0303)  REC_Loss: 0.0986 (0.0971)  dice: 1.3389 (1.2743)  time: 0.0143  data: 0.0001  max mem: 4345\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.3546 (0.6894)  CLS_Loss: 0.2546 (0.5598)  SEG_Loss: 0.0000 (0.0310)  REC_Loss: 0.1025 (0.0986)  dice: 1.2452 (1.2559)  time: 0.0143  data: 0.0001  max mem: 4345\n",
      "Valid: Total time: 0:00:01 (0.0249 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.6894331, 'CLS_Loss': 0.5598408, 'SEG_Loss': 0.0310394, 'REC_Loss': 0.0985529, 'dice': 1.2559432, 'auc': 0.4489796, 'f1': 0.0, 'acc': 0.7857143, 'sen': 0.0, 'spe': 0.9821429}\n",
      "Epoch: [3]  [ 0/33]  eta: 0:00:22  lr: 0.000100  loss: 0.8479 (0.8479)  CLS_Loss: 0.6614 (0.6614)  SEG_Loss: 0.0848 (0.0848)  REC_Loss: 0.1018 (0.1018)  time: 0.6928  data: 0.5431  max mem: 4349\n",
      "Epoch: [3]  [10/33]  eta: 0:00:04  lr: 0.000100  loss: 1.0947 (1.1036)  CLS_Loss: 0.6955 (0.6894)  SEG_Loss: 0.1869 (0.3120)  REC_Loss: 0.1025 (0.1023)  time: 0.2061  data: 0.0495  max mem: 4349\n",
      "Epoch: [3]  [20/33]  eta: 0:00:02  lr: 0.000100  loss: 1.0885 (1.0831)  CLS_Loss: 0.6663 (0.6537)  SEG_Loss: 0.3144 (0.3263)  REC_Loss: 0.1025 (0.1031)  time: 0.1580  data: 0.0002  max mem: 4349\n",
      "Epoch: [3]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 1.1189 (1.0922)  CLS_Loss: 0.5326 (0.6226)  SEG_Loss: 0.3144 (0.3682)  REC_Loss: 0.0997 (0.1015)  time: 0.1594  data: 0.0002  max mem: 4349\n",
      "Epoch: [3]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 1.2045 (1.0928)  CLS_Loss: 0.5438 (0.6335)  SEG_Loss: 0.3266 (0.3579)  REC_Loss: 0.0997 (0.1014)  time: 0.1596  data: 0.0002  max mem: 4349\n",
      "Epoch: [3] Total time: 0:00:05 (0.1804 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 1.0927591, 'CLS_Loss': 0.6334877, 'SEG_Loss': 0.3578827, 'REC_Loss': 0.1013887}\n",
      "Valid:  [ 0/70]  eta: 0:00:31  loss: 0.1895 (0.1895)  CLS_Loss: 0.0973 (0.0973)  SEG_Loss: 0.0000 (0.0000)  REC_Loss: 0.0922 (0.0922)  time: 0.4429  data: 0.4204  max mem: 4349\n",
      "Valid:  [10/70]  eta: 0:00:03  loss: 0.4276 (0.9728)  CLS_Loss: 0.3607 (0.8258)  SEG_Loss: 0.0000 (0.0572)  REC_Loss: 0.0955 (0.0898)  dice: 1.2116 (1.2153)  time: 0.0531  data: 0.0383  max mem: 4349\n",
      "Valid:  [20/70]  eta: 0:00:01  loss: 0.3157 (0.7719)  CLS_Loss: 0.2294 (0.6382)  SEG_Loss: 0.0000 (0.0397)  REC_Loss: 0.0957 (0.0940)  dice: 1.2172 (1.2586)  time: 0.0151  data: 0.0001  max mem: 4349\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.3109 (0.7045)  CLS_Loss: 0.2294 (0.5870)  SEG_Loss: 0.0000 (0.0211)  REC_Loss: 0.0978 (0.0964)  dice: 1.4162 (1.3380)  time: 0.0151  data: 0.0001  max mem: 4349\n",
      "Valid:  [40/70]  eta: 0:00:00  loss: 0.3287 (0.6811)  CLS_Loss: 0.2390 (0.5613)  SEG_Loss: 0.0000 (0.0233)  REC_Loss: 0.0969 (0.0965)  dice: 1.2172 (1.3003)  time: 0.0149  data: 0.0001  max mem: 4349\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.3325 (0.7384)  CLS_Loss: 0.2405 (0.6221)  SEG_Loss: 0.0000 (0.0218)  REC_Loss: 0.0890 (0.0945)  dice: 1.2172 (1.3385)  time: 0.0158  data: 0.0001  max mem: 4349\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.4155 (0.6862)  CLS_Loss: 0.3102 (0.5722)  SEG_Loss: 0.0000 (0.0182)  REC_Loss: 0.0898 (0.0958)  dice: 1.2172 (1.3385)  time: 0.0151  data: 0.0001  max mem: 4349\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.3445 (0.6740)  CLS_Loss: 0.2584 (0.5603)  SEG_Loss: 0.0000 (0.0189)  REC_Loss: 0.0933 (0.0948)  dice: 1.2172 (1.3221)  time: 0.0156  data: 0.0001  max mem: 4349\n",
      "Valid: Total time: 0:00:01 (0.0241 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.6739953, 'CLS_Loss': 0.560294, 'SEG_Loss': 0.0188895, 'REC_Loss': 0.0948117, 'dice': 1.3220595, 'auc': 0.432398, 'f1': 0.0, 'acc': 0.7857143, 'sen': 0.0, 'spe': 0.9821429}\n",
      "Epoch: [4]  [ 0/33]  eta: 0:00:22  lr: 0.000100  loss: 1.0843 (1.0843)  CLS_Loss: 0.9915 (0.9915)  SEG_Loss: -0.0085 (-0.0085)  REC_Loss: 0.1014 (0.1014)  time: 0.6669  data: 0.5101  max mem: 4349\n",
      "Epoch: [4]  [10/33]  eta: 0:00:04  lr: 0.000100  loss: 1.0843 (0.9969)  CLS_Loss: 0.7416 (0.6426)  SEG_Loss: 0.2950 (0.2603)  REC_Loss: 0.0937 (0.0940)  time: 0.2024  data: 0.0465  max mem: 4349\n",
      "Epoch: [4]  [20/33]  eta: 0:00:02  lr: 0.000100  loss: 1.1641 (1.0961)  CLS_Loss: 0.5798 (0.6854)  SEG_Loss: 0.2950 (0.3154)  REC_Loss: 0.0936 (0.0953)  time: 0.1571  data: 0.0002  max mem: 4349\n",
      "Epoch: [4]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 1.1641 (1.0933)  CLS_Loss: 0.6081 (0.6789)  SEG_Loss: 0.2440 (0.3194)  REC_Loss: 0.0936 (0.0950)  time: 0.1585  data: 0.0002  max mem: 4349\n",
      "Epoch: [4]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 1.1641 (1.0967)  CLS_Loss: 0.6117 (0.6889)  SEG_Loss: 0.3123 (0.3133)  REC_Loss: 0.0924 (0.0945)  time: 0.1586  data: 0.0001  max mem: 4349\n",
      "Epoch: [4] Total time: 0:00:05 (0.1783 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 1.096703, 'CLS_Loss': 0.6888579, 'SEG_Loss': 0.313333, 'REC_Loss': 0.0945121}\n",
      "Valid:  [ 0/70]  eta: 0:00:32  loss: 0.4948 (0.4948)  CLS_Loss: 0.4038 (0.4038)  SEG_Loss: 0.0000 (0.0000)  REC_Loss: 0.0910 (0.0910)  time: 0.4605  data: 0.4355  max mem: 4349\n",
      "Valid:  [10/70]  eta: 0:00:03  loss: 0.3831 (0.5083)  CLS_Loss: 0.3160 (0.4672)  SEG_Loss: 0.0000 (-0.0390)  REC_Loss: 0.0799 (0.0801)  dice: 1.6494 (1.7483)  time: 0.0550  data: 0.0397  max mem: 4349\n",
      "Valid:  [20/70]  eta: 0:00:01  loss: 0.3831 (0.6279)  CLS_Loss: 0.3160 (0.5317)  SEG_Loss: 0.0000 (0.0113)  REC_Loss: 0.0799 (0.0849)  dice: 1.2402 (1.2493)  time: 0.0143  data: 0.0001  max mem: 4349\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.5512 (0.6770)  CLS_Loss: 0.4465 (0.5829)  SEG_Loss: 0.0000 (0.0065)  REC_Loss: 0.0913 (0.0876)  dice: 1.3133 (1.3071)  time: 0.0142  data: 0.0001  max mem: 4349\n",
      "Valid:  [40/70]  eta: 0:00:00  loss: 0.4042 (0.6032)  CLS_Loss: 0.3050 (0.5090)  SEG_Loss: 0.0000 (0.0049)  REC_Loss: 0.0926 (0.0893)  dice: 1.3133 (1.3071)  time: 0.0143  data: 0.0001  max mem: 4349\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.3688 (0.5658)  CLS_Loss: 0.2784 (0.4741)  SEG_Loss: 0.0000 (0.0039)  REC_Loss: 0.0892 (0.0878)  dice: 1.3133 (1.3071)  time: 0.0150  data: 0.0001  max mem: 4349\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.4469 (0.6277)  CLS_Loss: 0.3665 (0.5240)  SEG_Loss: 0.0000 (0.0167)  REC_Loss: 0.0821 (0.0870)  dice: 1.2402 (1.2432)  time: 0.0153  data: 0.0001  max mem: 4349\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.3571 (0.5921)  CLS_Loss: 0.2868 (0.4908)  SEG_Loss: 0.0000 (0.0145)  REC_Loss: 0.0851 (0.0868)  dice: 1.2402 (1.2432)  time: 0.0142  data: 0.0001  max mem: 4349\n",
      "Valid: Total time: 0:00:01 (0.0235 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.5921168, 'CLS_Loss': 0.4908462, 'SEG_Loss': 0.0145187, 'REC_Loss': 0.0867519, 'dice': 1.2432258, 'auc': 0.6390306, 'f1': 0.0, 'acc': 0.8, 'sen': 0.0, 'spe': 1.0}\n",
      "Epoch: [5]  [ 0/33]  eta: 0:00:22  lr: 0.000100  loss: 1.0924 (1.0924)  CLS_Loss: 0.6380 (0.6380)  SEG_Loss: 0.3644 (0.3644)  REC_Loss: 0.0900 (0.0900)  time: 0.6808  data: 0.5258  max mem: 4349\n",
      "Epoch: [5]  [10/33]  eta: 0:00:04  lr: 0.000100  loss: 1.0924 (1.1402)  CLS_Loss: 0.5542 (0.6232)  SEG_Loss: 0.3894 (0.4284)  REC_Loss: 0.0900 (0.0886)  time: 0.2034  data: 0.0479  max mem: 4349\n",
      "Epoch: [5]  [20/33]  eta: 0:00:02  lr: 0.000100  loss: 0.9942 (1.0294)  CLS_Loss: 0.5279 (0.6123)  SEG_Loss: 0.3016 (0.3274)  REC_Loss: 0.0893 (0.0896)  time: 0.1565  data: 0.0001  max mem: 4349\n",
      "Epoch: [5]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.8457 (1.0178)  CLS_Loss: 0.4606 (0.5766)  SEG_Loss: 0.3095 (0.3514)  REC_Loss: 0.0894 (0.0899)  time: 0.1560  data: 0.0001  max mem: 4349\n",
      "Epoch: [5]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.8457 (0.9995)  CLS_Loss: 0.5170 (0.5871)  SEG_Loss: 0.3095 (0.3221)  REC_Loss: 0.0902 (0.0904)  time: 0.1557  data: 0.0001  max mem: 4349\n",
      "Epoch: [5] Total time: 0:00:05 (0.1770 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.9995114, 'CLS_Loss': 0.5870902, 'SEG_Loss': 0.3220582, 'REC_Loss': 0.0903631}\n",
      "Valid:  [ 0/70]  eta: 0:00:31  loss: 0.2240 (0.2240)  CLS_Loss: 0.1226 (0.1226)  SEG_Loss: 0.0000 (0.0000)  REC_Loss: 0.1014 (0.1014)  time: 0.4534  data: 0.4271  max mem: 4349\n",
      "Valid:  [10/70]  eta: 0:00:03  loss: 0.4626 (1.4656)  CLS_Loss: 0.3820 (1.4281)  SEG_Loss: 0.0000 (-0.0663)  REC_Loss: 0.1046 (0.1038)  dice: 1.3367 (1.4662)  time: 0.0569  data: 0.0389  max mem: 4349\n",
      "Valid:  [20/70]  eta: 0:00:01  loss: 0.2848 (1.0823)  CLS_Loss: 0.2074 (1.0212)  SEG_Loss: 0.0000 (-0.0458)  REC_Loss: 0.1046 (0.1069)  dice: 1.3367 (1.4660)  time: 0.0170  data: 0.0001  max mem: 4349\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.2433 (0.9322)  CLS_Loss: 0.1564 (0.8611)  SEG_Loss: 0.0000 (-0.0303)  REC_Loss: 0.0969 (0.1014)  dice: 1.3367 (1.4536)  time: 0.0154  data: 0.0001  max mem: 4349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid:  [40/70]  eta: 0:00:00  loss: 0.3479 (0.8900)  CLS_Loss: 0.2538 (0.8274)  SEG_Loss: 0.0000 (-0.0361)  REC_Loss: 0.0869 (0.0987)  dice: 1.3670 (1.4974)  time: 0.0141  data: 0.0001  max mem: 4349\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.3479 (0.8218)  CLS_Loss: 0.2538 (0.7606)  SEG_Loss: 0.0000 (-0.0359)  REC_Loss: 0.0868 (0.0971)  dice: 1.5132 (1.5299)  time: 0.0142  data: 0.0001  max mem: 4349\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.2688 (0.8130)  CLS_Loss: 0.1886 (0.7346)  SEG_Loss: 0.0000 (-0.0175)  REC_Loss: 0.0871 (0.0960)  dice: 1.3670 (1.4030)  time: 0.0147  data: 0.0001  max mem: 4349\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.2688 (0.7632)  CLS_Loss: 0.1759 (0.6810)  SEG_Loss: 0.0000 (-0.0139)  REC_Loss: 0.0914 (0.0961)  dice: 1.3367 (1.3856)  time: 0.0146  data: 0.0001  max mem: 4349\n",
      "Valid: Total time: 0:00:01 (0.0240 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.7631899, 'CLS_Loss': 0.6809594, 'SEG_Loss': -0.013901, 'REC_Loss': 0.0961315, 'dice': 1.3856257, 'auc': 0.2908163, 'f1': 0.0, 'acc': 0.8, 'sen': 0.0, 'spe': 1.0}\n",
      "Epoch: [6]  [ 0/33]  eta: 0:00:23  lr: 0.000100  loss: 0.6676 (0.6676)  CLS_Loss: 0.6936 (0.6936)  SEG_Loss: -0.1214 (-0.1214)  REC_Loss: 0.0954 (0.0954)  time: 0.6991  data: 0.5449  max mem: 4349\n",
      "Epoch: [6]  [10/33]  eta: 0:00:04  lr: 0.000100  loss: 0.9518 (0.9414)  CLS_Loss: 0.6266 (0.6102)  SEG_Loss: 0.2016 (0.2380)  REC_Loss: 0.0932 (0.0932)  time: 0.2064  data: 0.0497  max mem: 4349\n",
      "Epoch: [6]  [20/33]  eta: 0:00:02  lr: 0.000100  loss: 0.9892 (0.9721)  CLS_Loss: 0.5206 (0.5730)  SEG_Loss: 0.2511 (0.3063)  REC_Loss: 0.0930 (0.0928)  time: 0.1573  data: 0.0002  max mem: 4349\n",
      "Epoch: [6]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.7145 (0.9161)  CLS_Loss: 0.5206 (0.5796)  SEG_Loss: 0.1600 (0.2467)  REC_Loss: 0.0846 (0.0898)  time: 0.1579  data: 0.0001  max mem: 4349\n",
      "Epoch: [6]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.6910 (0.9038)  CLS_Loss: 0.4948 (0.5667)  SEG_Loss: 0.1485 (0.2477)  REC_Loss: 0.0845 (0.0893)  time: 0.1579  data: 0.0001  max mem: 4349\n",
      "Epoch: [6] Total time: 0:00:05 (0.1795 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.9037903, 'CLS_Loss': 0.5667358, 'SEG_Loss': 0.2477053, 'REC_Loss': 0.0893491}\n",
      "Valid:  [ 0/70]  eta: 0:00:32  loss: 3.6621 (3.6621)  CLS_Loss: 3.2598 (3.2598)  SEG_Loss: 0.3355 (0.3355)  REC_Loss: 0.0668 (0.0668)  dice: 0.9464 (0.9464)  time: 0.4608  data: 0.4371  max mem: 4349\n",
      "Valid:  [10/70]  eta: 0:00:03  loss: 0.2488 (0.7064)  CLS_Loss: 0.1662 (0.6900)  SEG_Loss: 0.0000 (-0.0576)  REC_Loss: 0.0693 (0.0741)  dice: 1.6391 (1.4803)  time: 0.0575  data: 0.0398  max mem: 4349\n",
      "Valid:  [20/70]  eta: 0:00:01  loss: 0.2804 (0.6315)  CLS_Loss: 0.2069 (0.5867)  SEG_Loss: 0.0000 (-0.0280)  REC_Loss: 0.0709 (0.0728)  dice: 1.0827 (1.3809)  time: 0.0169  data: 0.0001  max mem: 4349\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.2804 (0.5718)  CLS_Loss: 0.2069 (0.4922)  SEG_Loss: 0.0000 (0.0062)  REC_Loss: 0.0737 (0.0734)  dice: 1.0827 (1.1047)  time: 0.0156  data: 0.0001  max mem: 4349\n",
      "Valid:  [40/70]  eta: 0:00:00  loss: 0.2534 (0.5516)  CLS_Loss: 0.1787 (0.4868)  SEG_Loss: 0.0000 (-0.0089)  REC_Loss: 0.0757 (0.0737)  dice: 1.4735 (1.2114)  time: 0.0154  data: 0.0001  max mem: 4349\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.2534 (0.5168)  CLS_Loss: 0.1787 (0.4681)  SEG_Loss: 0.0000 (-0.0247)  REC_Loss: 0.0755 (0.0734)  dice: 1.4824 (1.3498)  time: 0.0170  data: 0.0001  max mem: 4349\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.2261 (0.4964)  CLS_Loss: 0.1556 (0.4418)  SEG_Loss: 0.0000 (-0.0187)  REC_Loss: 0.0726 (0.0732)  dice: 1.4735 (1.3174)  time: 0.0169  data: 0.0001  max mem: 4349\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.2592 (0.5247)  CLS_Loss: 0.1902 (0.4754)  SEG_Loss: 0.0000 (-0.0239)  REC_Loss: 0.0726 (0.0732)  dice: 1.2901 (1.3313)  time: 0.0155  data: 0.0001  max mem: 4349\n",
      "Valid: Total time: 0:00:01 (0.0249 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.5247096, 'CLS_Loss': 0.4754305, 'SEG_Loss': -0.0239274, 'REC_Loss': 0.0732065, 'dice': 1.3312811, 'auc': 0.6760204, 'f1': 0.0, 'acc': 0.8, 'sen': 0.0, 'spe': 1.0}\n",
      "Epoch: [7]  [ 0/33]  eta: 0:00:22  lr: 0.000100  loss: 1.3596 (1.3596)  CLS_Loss: 0.6047 (0.6047)  SEG_Loss: 0.6740 (0.6740)  REC_Loss: 0.0810 (0.0810)  time: 0.6667  data: 0.5115  max mem: 4349\n",
      "Epoch: [7]  [10/33]  eta: 0:00:04  lr: 0.000100  loss: 0.9445 (0.9067)  CLS_Loss: 0.6047 (0.5849)  SEG_Loss: 0.2716 (0.2402)  REC_Loss: 0.0810 (0.0816)  time: 0.2009  data: 0.0467  max mem: 4349\n",
      "Epoch: [7]  [20/33]  eta: 0:00:02  lr: 0.000100  loss: 0.8690 (0.9008)  CLS_Loss: 0.6279 (0.6242)  SEG_Loss: 0.2526 (0.1950)  REC_Loss: 0.0785 (0.0817)  time: 0.1549  data: 0.0002  max mem: 4349\n",
      "Epoch: [7]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.9401 (0.9201)  CLS_Loss: 0.6224 (0.6237)  SEG_Loss: 0.0486 (0.2143)  REC_Loss: 0.0785 (0.0821)  time: 0.1556  data: 0.0001  max mem: 4349\n",
      "Epoch: [7]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.9479 (0.9438)  CLS_Loss: 0.6224 (0.6413)  SEG_Loss: 0.2311 (0.2206)  REC_Loss: 0.0785 (0.0819)  time: 0.1558  data: 0.0001  max mem: 4349\n",
      "Epoch: [7] Total time: 0:00:05 (0.1766 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.9438026, 'CLS_Loss': 0.6412816, 'SEG_Loss': 0.2205807, 'REC_Loss': 0.0819403}\n",
      "Valid:  [ 0/70]  eta: 0:00:32  loss: 0.2014 (0.2014)  CLS_Loss: 0.1209 (0.1209)  SEG_Loss: 0.0000 (0.0000)  REC_Loss: 0.0805 (0.0805)  time: 0.4685  data: 0.4451  max mem: 4349\n",
      "Valid:  [10/70]  eta: 0:00:03  loss: 0.3678 (0.7423)  CLS_Loss: 0.2903 (0.8064)  SEG_Loss: 0.0000 (-0.1364)  REC_Loss: 0.0754 (0.0723)  dice: 1.4757 (1.5506)  time: 0.0583  data: 0.0406  max mem: 4349\n",
      "Valid:  [20/70]  eta: 0:00:01  loss: 0.3041 (0.5816)  CLS_Loss: 0.2338 (0.5840)  SEG_Loss: 0.0000 (-0.0741)  REC_Loss: 0.0712 (0.0716)  dice: 1.4757 (1.4879)  time: 0.0168  data: 0.0001  max mem: 4349\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.2625 (0.6221)  CLS_Loss: 0.1958 (0.5906)  SEG_Loss: 0.0000 (-0.0387)  REC_Loss: 0.0667 (0.0702)  dice: 1.2668 (1.3072)  time: 0.0153  data: 0.0001  max mem: 4349\n",
      "Valid:  [40/70]  eta: 0:00:00  loss: 0.2313 (0.6456)  CLS_Loss: 0.1564 (0.5974)  SEG_Loss: 0.0000 (-0.0231)  REC_Loss: 0.0721 (0.0713)  dice: 1.2374 (1.2211)  time: 0.0152  data: 0.0001  max mem: 4349\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.2107 (0.6262)  CLS_Loss: 0.1485 (0.5957)  SEG_Loss: 0.0000 (-0.0417)  REC_Loss: 0.0727 (0.0722)  dice: 1.2668 (1.3147)  time: 0.0171  data: 0.0001  max mem: 4349\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.2441 (0.5866)  CLS_Loss: 0.1723 (0.5492)  SEG_Loss: 0.0000 (-0.0355)  REC_Loss: 0.0754 (0.0729)  dice: 1.2374 (1.2943)  time: 0.0189  data: 0.0001  max mem: 4349\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.2271 (0.5722)  CLS_Loss: 0.1477 (0.5404)  SEG_Loss: 0.0000 (-0.0404)  REC_Loss: 0.0783 (0.0722)  dice: 1.2374 (1.3137)  time: 0.0177  data: 0.0001  max mem: 4349\n",
      "Valid: Total time: 0:00:01 (0.0255 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.5721953, 'CLS_Loss': 0.5403971, 'SEG_Loss': -0.040431, 'REC_Loss': 0.0722292, 'dice': 1.3137277, 'auc': 0.5076531, 'f1': 0.0, 'acc': 0.8, 'sen': 0.0, 'spe': 1.0}\n",
      "Epoch: [8]  [ 0/33]  eta: 0:00:19  lr: 0.000100  loss: 1.0417 (1.0417)  CLS_Loss: 0.9194 (0.9194)  SEG_Loss: 0.0526 (0.0526)  REC_Loss: 0.0696 (0.0696)  time: 0.5987  data: 0.4421  max mem: 4349\n",
      "Epoch: [8]  [10/33]  eta: 0:00:04  lr: 0.000100  loss: 0.8097 (0.7804)  CLS_Loss: 0.6377 (0.5984)  SEG_Loss: -0.0799 (0.1033)  REC_Loss: 0.0758 (0.0786)  time: 0.1947  data: 0.0403  max mem: 4349\n",
      "Epoch: [8]  [20/33]  eta: 0:00:02  lr: 0.000100  loss: 0.8097 (0.8479)  CLS_Loss: 0.4957 (0.5700)  SEG_Loss: 0.0494 (0.2022)  REC_Loss: 0.0757 (0.0757)  time: 0.1556  data: 0.0001  max mem: 4349\n",
      "Epoch: [8]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.7774 (0.7843)  CLS_Loss: 0.4468 (0.5317)  SEG_Loss: 0.1296 (0.1773)  REC_Loss: 0.0730 (0.0753)  time: 0.1566  data: 0.0001  max mem: 4349\n",
      "Epoch: [8]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.7814 (0.8011)  CLS_Loss: 0.4447 (0.5435)  SEG_Loss: 0.1296 (0.1814)  REC_Loss: 0.0728 (0.0762)  time: 0.1560  data: 0.0001  max mem: 4349\n",
      "Epoch: [8] Total time: 0:00:05 (0.1745 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.8011309, 'CLS_Loss': 0.5435335, 'SEG_Loss': 0.18142, 'REC_Loss': 0.0761774}\n",
      "Valid:  [ 0/70]  eta: 0:00:32  loss: 0.2597 (0.2597)  CLS_Loss: 0.2039 (0.2039)  SEG_Loss: 0.0000 (0.0000)  REC_Loss: 0.0558 (0.0558)  time: 0.4687  data: 0.4467  max mem: 4349\n",
      "Valid:  [10/70]  eta: 0:00:03  loss: 0.4145 (0.8132)  CLS_Loss: 0.3352 (0.7441)  SEG_Loss: 0.0000 (0.0022)  REC_Loss: 0.0659 (0.0669)  dice: 1.0200 (1.1211)  time: 0.0560  data: 0.0407  max mem: 4349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid:  [20/70]  eta: 0:00:01  loss: 0.3639 (0.6602)  CLS_Loss: 0.2819 (0.5990)  SEG_Loss: 0.0000 (-0.0058)  REC_Loss: 0.0659 (0.0670)  dice: 1.1942 (1.1525)  time: 0.0157  data: 0.0001  max mem: 4349\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.3295 (0.6471)  CLS_Loss: 0.2675 (0.6154)  SEG_Loss: 0.0000 (-0.0379)  REC_Loss: 0.0706 (0.0697)  dice: 1.2187 (1.3061)  time: 0.0172  data: 0.0001  max mem: 4349\n",
      "Valid:  [40/70]  eta: 0:00:00  loss: 0.3283 (0.5813)  CLS_Loss: 0.2555 (0.5570)  SEG_Loss: 0.0000 (-0.0444)  REC_Loss: 0.0665 (0.0686)  dice: 1.2783 (1.3675)  time: 0.0166  data: 0.0001  max mem: 4349\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.2523 (0.5173)  CLS_Loss: 0.1773 (0.4838)  SEG_Loss: 0.0000 (-0.0357)  REC_Loss: 0.0680 (0.0692)  dice: 1.2783 (1.3675)  time: 0.0152  data: 0.0001  max mem: 4349\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.2420 (0.5353)  CLS_Loss: 0.1607 (0.5009)  SEG_Loss: 0.0000 (-0.0353)  REC_Loss: 0.0719 (0.0697)  dice: 1.2187 (1.3329)  time: 0.0148  data: 0.0001  max mem: 4349\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.2175 (0.5605)  CLS_Loss: 0.1441 (0.5276)  SEG_Loss: 0.0000 (-0.0367)  REC_Loss: 0.0736 (0.0697)  dice: 1.2187 (1.3330)  time: 0.0146  data: 0.0001  max mem: 4349\n",
      "Valid: Total time: 0:00:01 (0.0243 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.5605304, 'CLS_Loss': 0.5275973, 'SEG_Loss': -0.036725, 'REC_Loss': 0.0696581, 'dice': 1.3329965, 'auc': 0.5357143, 'f1': 0.0, 'acc': 0.8, 'sen': 0.0, 'spe': 1.0}\n",
      "Epoch: [9]  [ 0/33]  eta: 0:00:21  lr: 0.000100  loss: 0.7183 (0.7183)  CLS_Loss: 0.6915 (0.6915)  SEG_Loss: -0.0420 (-0.0420)  REC_Loss: 0.0688 (0.0688)  time: 0.6550  data: 0.5045  max mem: 4349\n",
      "Epoch: [9]  [10/33]  eta: 0:00:04  lr: 0.000100  loss: 0.5338 (0.5606)  CLS_Loss: 0.5320 (0.4849)  SEG_Loss: 0.0000 (-0.0006)  REC_Loss: 0.0772 (0.0763)  time: 0.2000  data: 0.0460  max mem: 4349\n",
      "Epoch: [9]  [20/33]  eta: 0:00:02  lr: 0.000100  loss: 0.5512 (0.6920)  CLS_Loss: 0.5320 (0.5533)  SEG_Loss: 0.0955 (0.0635)  REC_Loss: 0.0757 (0.0752)  time: 0.1553  data: 0.0002  max mem: 4349\n",
      "Epoch: [9]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.9305 (0.7513)  CLS_Loss: 0.5359 (0.5622)  SEG_Loss: 0.1663 (0.1136)  REC_Loss: 0.0747 (0.0754)  time: 0.1563  data: 0.0001  max mem: 4349\n",
      "Epoch: [9]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.9659 (0.7996)  CLS_Loss: 0.6088 (0.5623)  SEG_Loss: 0.2012 (0.1620)  REC_Loss: 0.0747 (0.0753)  time: 0.1565  data: 0.0001  max mem: 4349\n",
      "Epoch: [9] Total time: 0:00:05 (0.1768 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.7995891, 'CLS_Loss': 0.5622538, 'SEG_Loss': 0.1620093, 'REC_Loss': 0.075326}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7fa2d10893a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 223, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import torch\n",
    "import utils\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from metrics import *\n",
    "from losses import soft_dice_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print_freq = 10\n",
    "output_dir = './checkpoints/mtl_2/'\n",
    "device     = 'cuda'\n",
    "\n",
    "# Whole LOOP\n",
    "for epoch in range(0, 200):\n",
    "    \n",
    "    ################################################################################################\n",
    "    # Training \n",
    "    ################################################################################################\n",
    "    \n",
    "    model.train(True)\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \", n=10)\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    \n",
    "    for batch_data in metric_logger.log_every(data_loader_train, print_freq, header):\n",
    "        \n",
    "        input   = batch_data[0].to(device).float()\n",
    "        seg_gt  = batch_data[1].to(device).float()\n",
    "        cls_gt  = seg_gt.flatten(1).bool().any(dim=1).float().unsqueeze(1)\n",
    "        \n",
    "        cls_pred, seg_pred, rec_pred = model(input)\n",
    "\n",
    "        loss, loss_detail = criterion(cls_pred=cls_pred, seg_pred=seg_pred, rec_pred=rec_pred, cls_gt=cls_gt, seg_gt=seg_gt, rec_gt=input)\n",
    "        \n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        metric_logger.update(loss=loss_value)\n",
    "        if loss_detail is not None:\n",
    "            metric_logger.update(**loss_detail)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "    \n",
    "    train_stats = {k: round(meter.global_avg, 7) for k, meter in metric_logger.meters.items()}\n",
    "    print(\"Averaged train_stats: \", train_stats)\n",
    "    \n",
    "    ################################################################################################\n",
    "    # Validation\n",
    "    ################################################################################################\n",
    "    \n",
    "    model.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \", n=1)\n",
    "    header = 'Valid:'\n",
    "\n",
    "    total_cls_pred  = torch.tensor([])\n",
    "    total_cls_true  = torch.tensor([])\n",
    "    \n",
    "    for batch_data in metric_logger.log_every(data_loader_valid, print_freq, header):\n",
    "        \n",
    "        input   = batch_data[0].to(device).float()\n",
    "        seg_gt  = batch_data[1].to(device).float()\n",
    "        cls_gt  = seg_gt.flatten(1).bool().any(dim=1).float().unsqueeze(1)\n",
    "\n",
    "        cls_pred, seg_pred, rec_pred = model(input)\n",
    "\n",
    "        loss, loss_detail = criterion(cls_pred=cls_pred, seg_pred=seg_pred, rec_pred=rec_pred, cls_gt=cls_gt, seg_gt=seg_gt, rec_gt=input)\n",
    "    \n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "\n",
    "        # LOSS\n",
    "        metric_logger.update(loss=loss_value)        \n",
    "        if loss_detail is not None:\n",
    "            metric_logger.update(**loss_detail)\n",
    "\n",
    "        # Post-processing\n",
    "        cls_pred = torch.sigmoid(cls_pred)\n",
    "        seg_pred = torch.sigmoid(seg_pred)\n",
    "\n",
    "        total_cls_pred  = torch.cat([total_cls_pred, cls_pred.detach().cpu()])\n",
    "        total_cls_true  = torch.cat([total_cls_true, cls_gt.detach().cpu()])\n",
    "\n",
    "        # Metrics SEG\n",
    "        if seg_gt.any():\n",
    "            dice = soft_dice_score(output=seg_pred.round(), target=seg_gt, smooth=0.0)    # pred_seg must be round() !! \n",
    "            metric_logger.update(dice=dice.item())     \n",
    "\n",
    "    # Metric CLS\n",
    "    auc            = roc_auc_score(y_true=total_cls_true, y_score=total_cls_pred)\n",
    "    tp, fp, fn, tn = get_stats(total_cls_pred.round().long(), total_cls_true.long(), mode=\"binary\")        \n",
    "    f1             = f1_score(tp, fp, fn, tn, reduction=\"macro\")\n",
    "    acc            = accuracy(tp, fp, fn, tn, reduction=\"macro\")\n",
    "    sen            = sensitivity(tp, fp, fn, tn, reduction=\"macro\")\n",
    "    spe            = specificity(tp, fp, fn, tn, reduction=\"macro\")\n",
    "\n",
    "    metric_logger.update(auc=auc, f1=f1, acc=acc, sen=sen, spe=spe)          \n",
    "    \n",
    "    valid_stats = {k: round(meter.global_avg, 7) for k, meter in metric_logger.meters.items()}\n",
    "    print(\"Averaged valid_stats: \", valid_stats)\n",
    "    \n",
    "    ################################################################################################\n",
    "    # Save & Log\n",
    "    ################################################################################################\n",
    "    \n",
    "    checkpoint_paths = output_dir + '/epoch_' + str(epoch) + '_checkpoint.pth'\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "    }, checkpoint_paths)\n",
    "\n",
    "    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                **{f'valid_{k}': v for k, v in valid_stats.items()},\n",
    "                'epoch': epoch}\n",
    "\n",
    "    if output_dir:\n",
    "        with open(output_dir + \"/log.txt\", \"a\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_log(path):\n",
    "    log_list = []\n",
    "    lines = open(path, 'r').read().splitlines() \n",
    "    for i in range(len(lines)):\n",
    "        exec('log_list.append('+lines[i] + ')')\n",
    "    return  log_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "51, 36, 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_list = read_log(path = '/workspace/sunggu/6.Kakao/checkpoints/[Baseline]Face_Net_DAC_fold0_upsample/log.txt')\n",
    "print(log_list[0].keys())\n",
    "result_dict = {}\n",
    "\n",
    "for key in log_list[0].keys():\n",
    "    exec( \"result_dict['\"+str(key)+\"']\" + \" = [ log_list[i]['\"+str(key)+\"'] for i in range(len(log_list)) ]\")\n",
    "\n",
    "for key in result_dict.keys():\n",
    "    plt.plot(result_dict[key])\n",
    "    plt.title(key)\n",
    "    print(\"###########################################################\")\n",
    "    print(\"Argsort = \", np.argsort(result_dict[key])[:5])\n",
    "    print(\"Value   = \", [result_dict[key][i] for i in np.argsort(result_dict[key])[:5]])\n",
    "    plt.show()\n",
    "    \n",
    "    if key == 'valid_loss':\n",
    "        print(\"Valid_Loss = \", np.argsort(result_dict[key])[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log check\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_log(path):\n",
    "    log_list = []\n",
    "    lines = open(path, 'r').read().splitlines() \n",
    "    for i in range(len(lines)):\n",
    "        exec('log_list.append('+lines[i] + ')')\n",
    "    return  log_list\n",
    "\n",
    "51, 36, 35\n",
    "\n",
    "log_list = read_log(path = '/workspace/sunggu/6.Kakao/checkpoints/[Baseline]Face_Net_DAC_fold0_upsample/log.txt')\n",
    "print(log_list[0].keys())\n",
    "result_dict = {}\n",
    "\n",
    "for key in log_list[0].keys():\n",
    "    exec( \"result_dict['\"+str(key)+\"']\" + \" = [ log_list[i]['\"+str(key)+\"'] for i in range(len(log_list)) ]\")\n",
    "\n",
    "for key in result_dict.keys():\n",
    "    plt.plot(result_dict[key])\n",
    "    plt.title(key)\n",
    "    print(\"###########################################################\")\n",
    "    print(\"Argsort = \", np.argsort(result_dict[key])[:5])\n",
    "    print(\"Value   = \", [result_dict[key][i] for i in np.argsort(result_dict[key])[:5]])\n",
    "    plt.show()\n",
    "    \n",
    "    if key == 'valid_loss':\n",
    "        print(\"Valid_Loss = \", np.argsort(result_dict[key])[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "\n",
    "print(\"Loading... Resume\")\n",
    "checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])        \n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])        \n",
    "args.start_epoch = checkpoint['epoch'] + 1  \n",
    "try:\n",
    "    log_path = os.path.dirname(args.resume)+'/log.txt'\n",
    "    lines    = open(log_path,'r').readlines()\n",
    "    val_loss_list = []\n",
    "    for l in lines:\n",
    "        exec('log_dict='+l.replace('NaN', '0'))\n",
    "        val_loss_list.append(log_dict['valid_loss'])\n",
    "    print(\"Epoch: \", np.argmin(val_loss_list), \" Minimum Val Loss ==> \", np.min(val_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "STL_cls.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
