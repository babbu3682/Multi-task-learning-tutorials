{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t85Nx2fQEF_s",
    "outputId": "09968e41-cfb5-47bd-8e6d-4a4a85cf2bbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 16 19:29:12 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:18:00.0 Off |                  Off |\n",
      "| 59%   87C    P2   277W / 300W |  10311MiB / 48685MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    Off  | 00000000:3B:00.0 Off |                  Off |\n",
      "|100%   87C    P2   262W / 300W |  45210MiB / 48685MiB |     47%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    Off  | 00000000:86:00.0 Off |                  Off |\n",
      "| 58%   75C    P8    46W / 300W |      3MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000    Off  | 00000000:AF:00.0 Off |                  Off |\n",
      "| 68%   84C    P2   232W / 300W |  25408MiB / 48685MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "import cv2\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P: 75 (22.73%) N: 255 (77.27%) Total: 330\n",
    "P: 14 (20.00%) N: 56  (80.00%) Total: 70\n",
    "P: 21 (21.00%) N: 79  (79.00%) Total: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/sunggu/7.KOHI/Multi_task_learning_tutorials'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CN3SCwTVEF_v"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = './SSIM_seg/'\n",
    "\n",
    "x_train_list = glob.glob(os.path.join(DATA_DIR, 'train/*'))\n",
    "y_train_list = glob.glob(os.path.join(DATA_DIR, 'trainannot/*'))\n",
    "\n",
    "x_valid_list = glob.glob(os.path.join(DATA_DIR, 'val/*'))\n",
    "y_valid_list = glob.glob(os.path.join(DATA_DIR, 'valannot/*'))\n",
    "\n",
    "x_test_list  = glob.glob(os.path.join(DATA_DIR, 'test/*'))\n",
    "y_test_list  = glob.glob(os.path.join(DATA_DIR, 'testannot/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./SSIM_seg/train/1.2.276.0.7230010.3.1.4.8323329.300.1517875162.258081.png'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./SSIM_seg/val/1.2.276.0.7230010.3.1.4.8323329.1173.1517875166.626582.png'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "f0iPTFh_EF_w"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Dataset Class\n",
    "class Dataset(BaseDataset):    \n",
    "    def __init__(self, images_list, labels_list, transform):\n",
    "        self.images_list  = images_list\n",
    "        self.labels_list  = labels_list\n",
    "        self.transform    = transform\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # read data\n",
    "        image = cv2.imread(self.images_list[i], cv2.IMREAD_GRAYSCALE)\n",
    "        mask  = cv2.imread(self.labels_list[i], cv2.IMREAD_GRAYSCALE)\n",
    "        path  = self.images_list[i]\n",
    "        \n",
    "        mask  = np.expand_dims(mask, axis=0)\n",
    "            \n",
    "        # apply transform\n",
    "        sample = self.transform(image=image, mask=mask)\n",
    "        image, mask = sample['image'], sample['mask']        \n",
    "        \n",
    "        return image, mask, path\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     12,
     35
    ]
   },
   "outputs": [],
   "source": [
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import albumentations as albu\n",
    "\n",
    "\n",
    "def minmax_normalize(image, **kwargs):\n",
    "    if len(np.unique(image)) != 1:  # Sometimes it cause the nan inputs...\n",
    "        image = image.astype('float32')\n",
    "        image -= image.min()\n",
    "        image /= image.max() \n",
    "    return image\n",
    "\n",
    "\n",
    "train_transform = albu.Compose([\n",
    "    albu.HorizontalFlip(p=0.5),\n",
    "    albu.ShiftScaleRotate(scale_limit=0.10, shift_limit=0.10, rotate_limit=15, p=0.5),\n",
    "    albu.GaussNoise(p=0.2),\n",
    "    albu.OneOf(\n",
    "        [\n",
    "            albu.CLAHE(p=1),\n",
    "            albu.RandomBrightnessContrast(p=1),\n",
    "            albu.RandomGamma(p=1),\n",
    "        ],\n",
    "        p=0.3,\n",
    "    ),\n",
    "    albu.OneOf(\n",
    "        [\n",
    "            albu.Blur(blur_limit=3, p=1),\n",
    "            albu.MotionBlur(blur_limit=3, p=1),\n",
    "        ],\n",
    "        p=0.3,\n",
    "    ),\n",
    "    albu.Lambda(image=minmax_normalize, always_apply=True),\n",
    "    ToTensorV2(),    \n",
    "])\n",
    "\n",
    "valid_transform = albu.Compose([        \n",
    "    albu.Lambda(image=minmax_normalize, always_apply=True),\n",
    "    ToTensorV2(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset(images_list=x_train_list, labels_list=y_train_list, transform=train_transform)\n",
    "dataset_valid = Dataset(images_list=x_valid_list, labels_list=y_valid_list, transform=valid_transform)\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=10, num_workers=4, shuffle=True, pin_memory=True, drop_last=True)\n",
    "data_loader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=1,  num_workers=4, shuffle=True, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "cdMO4sJyEF_z",
    "outputId": "503b9008-e50a-41e5-bf09-ce4576d83d5f"
   },
   "outputs": [],
   "source": [
    "# # same image with different random transforms\n",
    "\n",
    "# batch = next(iter(train_loader))\n",
    "# x = batch['x'][0]\n",
    "# y_seg = batch['y_seg'][0]\n",
    "# y_cls = batch['y_cls'][0]\n",
    "\n",
    "# print(x.shape,y_seg.shape,y_cls.shape)\n",
    "# print(torch.unique(y_seg),y_cls)\n",
    "# visualize(image=x, mask=y_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Learnable Params: 14322711\n"
     ]
    }
   ],
   "source": [
    "from arch.smart_net import *\n",
    "from losses import MTL_Loss\n",
    "\n",
    "# Model\n",
    "model        = MTL_1_Net(encoder_name='resnet18').to('cuda')     \n",
    "\n",
    "# Loss\n",
    "criterion    = MTL_Loss(name='MTL_1')\n",
    "\n",
    "# Optimizer & LR Schedule   \n",
    "optimizer    = torch.optim.AdamW(params=model.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-4, amsgrad=False)\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Number of Learnable Params:', n_parameters)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "id": "ojQz_ZNgEF_0",
    "outputId": "ca6fe927-248f-42af-e694-b2b740f286e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/33]  eta: 0:00:29  lr: 0.000100  loss: 1.0010 (1.0010)  CLS_Loss: 0.7267 (0.7267)  SEG_Loss: 0.2744 (0.2744)  time: 0.8952  data: 0.5888  max mem: 2750\n",
      "Epoch: [0]  [10/33]  eta: 0:00:06  lr: 0.000100  loss: 0.7641 (0.8349)  CLS_Loss: 0.6164 (0.6142)  SEG_Loss: 0.1442 (0.2207)  time: 0.2833  data: 0.0536  max mem: 2917\n",
      "Epoch: [0]  [20/33]  eta: 0:00:03  lr: 0.000100  loss: 0.9044 (0.9341)  CLS_Loss: 0.5497 (0.5999)  SEG_Loss: 0.2480 (0.3342)  time: 0.2513  data: 0.0002  max mem: 2917\n",
      "Epoch: [0]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.9044 (0.8909)  CLS_Loss: 0.5453 (0.5730)  SEG_Loss: 0.3503 (0.3179)  time: 0.2579  data: 0.0002  max mem: 2917\n",
      "Epoch: [0]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.9044 (0.8999)  CLS_Loss: 0.4534 (0.5686)  SEG_Loss: 0.4179 (0.3313)  time: 0.2545  data: 0.0001  max mem: 2917\n",
      "Epoch: [0] Total time: 0:00:08 (0.2681 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.8999225, 'CLS_Loss': 0.5685978, 'SEG_Loss': 0.3313247}\n",
      "Valid:  [ 0/70]  eta: 0:00:31  loss: 0.2038 (0.2038)  CLS_Loss: 0.2038 (0.2038)  SEG_Loss: 0.0000 (0.0000)  time: 0.4499  data: 0.4202  max mem: 2917\n",
      "Valid:  [10/70]  eta: 0:00:03  loss: 0.2041 (0.3393)  CLS_Loss: 0.2041 (0.3394)  SEG_Loss: 0.0000 (-0.0002)  dice: 1.2487 (1.2487)  time: 0.0595  data: 0.0384  max mem: 2917\n",
      "Valid:  [20/70]  eta: 0:00:02  loss: 0.2047 (0.3256)  CLS_Loss: 0.2047 (0.3374)  SEG_Loss: 0.0000 (-0.0118)  dice: 1.2487 (1.5300)  time: 0.0230  data: 0.0002  max mem: 2917\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.2093 (0.4402)  CLS_Loss: 0.2093 (0.4309)  SEG_Loss: 0.0000 (0.0093)  dice: 1.2487 (1.2714)  time: 0.0267  data: 0.0001  max mem: 2917\n",
      "Valid:  [40/70]  eta: 0:00:01  loss: 0.2142 (0.4866)  CLS_Loss: 0.2142 (0.4813)  SEG_Loss: 0.0000 (0.0054)  dice: 1.2487 (1.2996)  time: 0.0288  data: 0.0001  max mem: 2917\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.2165 (0.5496)  CLS_Loss: 0.2165 (0.5406)  SEG_Loss: 0.0000 (0.0091)  dice: 1.2487 (1.2886)  time: 0.0298  data: 0.0001  max mem: 2917\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.2154 (0.5227)  CLS_Loss: 0.2154 (0.5116)  SEG_Loss: 0.0000 (0.0111)  dice: 1.2487 (1.2688)  time: 0.0244  data: 0.0001  max mem: 2917\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.2136 (0.5019)  CLS_Loss: 0.2136 (0.4939)  SEG_Loss: 0.0000 (0.0080)  dice: 1.2487 (1.2796)  time: 0.0193  data: 0.0001  max mem: 2917\n",
      "Valid: Total time: 0:00:02 (0.0330 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.5018939, 'CLS_Loss': 0.4938545, 'SEG_Loss': 0.0080394, 'dice': 1.2795665, 'auc': 0.6109694, 'f1': 0.0, 'acc': 0.8, 'sen': 0.0, 'spe': 1.0}\n",
      "Epoch: [1]  [ 0/33]  eta: 0:00:18  lr: 0.000100  loss: 1.2876 (1.2876)  CLS_Loss: 0.4177 (0.4177)  SEG_Loss: 0.8699 (0.8699)  time: 0.5646  data: 0.4738  max mem: 3309\n",
      "Epoch: [1]  [10/33]  eta: 0:00:04  lr: 0.000100  loss: 0.7488 (0.8258)  CLS_Loss: 0.4336 (0.5656)  SEG_Loss: 0.1269 (0.2602)  time: 0.1740  data: 0.0432  max mem: 3309\n",
      "Epoch: [1]  [20/33]  eta: 0:00:02  lr: 0.000100  loss: 0.7793 (0.8340)  CLS_Loss: 0.5233 (0.5603)  SEG_Loss: 0.1269 (0.2737)  time: 0.1560  data: 0.0002  max mem: 3309\n",
      "Epoch: [1]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.8477 (0.8587)  CLS_Loss: 0.5263 (0.5481)  SEG_Loss: 0.3657 (0.3106)  time: 0.2305  data: 0.0002  max mem: 3309\n",
      "Epoch: [1]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.8477 (0.8685)  CLS_Loss: 0.5328 (0.5542)  SEG_Loss: 0.3657 (0.3143)  time: 0.2523  data: 0.0002  max mem: 3309\n",
      "Epoch: [1] Total time: 0:00:07 (0.2223 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.8685336, 'CLS_Loss': 0.5542305, 'SEG_Loss': 0.3143031}\n",
      "Valid:  [ 0/70]  eta: 0:00:28  loss: 0.2456 (0.2456)  CLS_Loss: 0.2456 (0.2456)  SEG_Loss: 0.0000 (0.0000)  time: 0.4123  data: 0.3795  max mem: 3309\n",
      "Valid:  [10/70]  eta: 0:00:03  loss: 0.2904 (0.4176)  CLS_Loss: 0.2904 (0.5523)  SEG_Loss: 0.0000 (-0.1347)  dice: 1.5915 (1.6491)  time: 0.0665  data: 0.0346  max mem: 3309\n",
      "Valid:  [20/70]  eta: 0:00:02  loss: 0.3261 (0.3976)  CLS_Loss: 0.3261 (0.5056)  SEG_Loss: 0.0000 (-0.1080)  dice: 1.5915 (1.6177)  time: 0.0309  data: 0.0001  max mem: 3309\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.3333 (0.4104)  CLS_Loss: 0.3333 (0.4873)  SEG_Loss: 0.0000 (-0.0769)  dice: 1.5193 (1.5479)  time: 0.0305  data: 0.0001  max mem: 3309\n",
      "Valid:  [40/70]  eta: 0:00:01  loss: 0.3333 (0.4148)  CLS_Loss: 0.3333 (0.4724)  SEG_Loss: 0.0000 (-0.0576)  dice: 1.5193 (1.4732)  time: 0.0291  data: 0.0001  max mem: 3309\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.3070 (0.4264)  CLS_Loss: 0.3070 (0.4692)  SEG_Loss: 0.0000 (-0.0428)  dice: 1.3080 (1.4117)  time: 0.0269  data: 0.0001  max mem: 3309\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.3648 (0.4623)  CLS_Loss: 0.3648 (0.5045)  SEG_Loss: 0.0000 (-0.0422)  dice: 1.3080 (1.3525)  time: 0.0285  data: 0.0001  max mem: 3309\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.3648 (0.4629)  CLS_Loss: 0.3648 (0.5045)  SEG_Loss: 0.0000 (-0.0416)  dice: 1.3080 (1.3436)  time: 0.0299  data: 0.0001  max mem: 3309\n",
      "Valid: Total time: 0:00:02 (0.0371 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.4628966, 'CLS_Loss': 0.5044552, 'SEG_Loss': -0.0415585, 'dice': 1.3435535, 'auc': 0.6428571, 'f1': 0.0, 'acc': 0.8, 'sen': 0.0, 'spe': 1.0}\n",
      "Epoch: [2]  [ 0/33]  eta: 0:00:17  lr: 0.000100  loss: 0.4217 (0.4217)  CLS_Loss: 0.3794 (0.3794)  SEG_Loss: 0.0424 (0.0424)  time: 0.5327  data: 0.4438  max mem: 3309\n",
      "Epoch: [2]  [10/33]  eta: 0:00:03  lr: 0.000100  loss: 0.8672 (0.7463)  CLS_Loss: 0.4109 (0.4366)  SEG_Loss: 0.2896 (0.3097)  time: 0.1391  data: 0.0405  max mem: 3309\n",
      "Epoch: [2]  [20/33]  eta: 0:00:01  lr: 0.000100  loss: 0.8544 (0.7977)  CLS_Loss: 0.4735 (0.5244)  SEG_Loss: 0.2896 (0.2733)  time: 0.1240  data: 0.0002  max mem: 3309\n",
      "Epoch: [2]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.7612 (0.7391)  CLS_Loss: 0.6245 (0.5629)  SEG_Loss: 0.0884 (0.1762)  time: 0.2333  data: 0.0002  max mem: 3309\n",
      "Epoch: [2]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.6487 (0.7085)  CLS_Loss: 0.5701 (0.5604)  SEG_Loss: 0.0232 (0.1481)  time: 0.2550  data: 0.0002  max mem: 3309\n",
      "Epoch: [2] Total time: 0:00:06 (0.2116 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.7085401, 'CLS_Loss': 0.5604438, 'SEG_Loss': 0.1480963}\n",
      "Valid:  [ 0/70]  eta: 0:00:36  loss: 0.3493 (0.3493)  CLS_Loss: 0.3493 (0.3493)  SEG_Loss: 0.0000 (0.0000)  time: 0.5169  data: 0.4832  max mem: 3309\n",
      "Valid:  [10/70]  eta: 0:00:04  loss: 0.4083 (0.4245)  CLS_Loss: 0.5394 (0.5091)  SEG_Loss: 0.0000 (-0.0845)  dice: 1.3160 (1.5691)  time: 0.0737  data: 0.0441  max mem: 3309\n",
      "Valid:  [20/70]  eta: 0:00:02  loss: 0.4464 (0.5059)  CLS_Loss: 0.5394 (0.5672)  SEG_Loss: 0.0000 (-0.0613)  dice: 1.3160 (1.4865)  time: 0.0290  data: 0.0002  max mem: 3309\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.4464 (0.5170)  CLS_Loss: 0.4707 (0.5755)  SEG_Loss: 0.0000 (-0.0585)  dice: 1.3160 (1.4117)  time: 0.0297  data: 0.0001  max mem: 3309\n",
      "Valid:  [40/70]  eta: 0:00:01  loss: 0.3853 (0.5159)  CLS_Loss: 0.4766 (0.5761)  SEG_Loss: 0.0000 (-0.0603)  dice: 1.3160 (1.3313)  time: 0.0302  data: 0.0001  max mem: 3309\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.4518 (0.5192)  CLS_Loss: 0.4766 (0.5677)  SEG_Loss: 0.0000 (-0.0484)  dice: 1.3160 (1.3313)  time: 0.0286  data: 0.0001  max mem: 3309\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.4189 (0.4910)  CLS_Loss: 0.4518 (0.5516)  SEG_Loss: 0.0000 (-0.0606)  dice: 1.3160 (1.3528)  time: 0.0283  data: 0.0001  max mem: 3309\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.3884 (0.4869)  CLS_Loss: 0.4512 (0.5449)  SEG_Loss: 0.0000 (-0.0580)  dice: 1.3160 (1.3576)  time: 0.0292  data: 0.0001  max mem: 3309\n",
      "Valid: Total time: 0:00:02 (0.0383 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.4869228, 'CLS_Loss': 0.5449158, 'SEG_Loss': -0.057993, 'dice': 1.3576396, 'auc': 0.622449, 'f1': 0.2, 'acc': 0.7714286, 'sen': 0.1428571, 'spe': 0.9285714}\n",
      "Epoch: [3]  [ 0/33]  eta: 0:00:18  lr: 0.000100  loss: 1.1294 (1.1294)  CLS_Loss: 0.9963 (0.9963)  SEG_Loss: 0.1331 (0.1331)  time: 0.5532  data: 0.4672  max mem: 3309\n",
      "Epoch: [3]  [10/33]  eta: 0:00:03  lr: 0.000100  loss: 0.6955 (0.7119)  CLS_Loss: 0.5640 (0.5967)  SEG_Loss: 0.1331 (0.1152)  time: 0.1407  data: 0.0426  max mem: 3309\n",
      "Epoch: [3]  [20/33]  eta: 0:00:02  lr: 0.000100  loss: 0.6538 (0.7325)  CLS_Loss: 0.5480 (0.5911)  SEG_Loss: 0.1056 (0.1414)  time: 0.1604  data: 0.0001  max mem: 3309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.6760 (0.7243)  CLS_Loss: 0.4935 (0.5505)  SEG_Loss: 0.2181 (0.1738)  time: 0.2701  data: 0.0001  max mem: 3309\n",
      "Epoch: [3]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.6512 (0.6982)  CLS_Loss: 0.4989 (0.5469)  SEG_Loss: 0.0982 (0.1513)  time: 0.2920  data: 0.0001  max mem: 3309\n",
      "Epoch: [3] Total time: 0:00:07 (0.2352 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.6981643, 'CLS_Loss': 0.5468816, 'SEG_Loss': 0.1512827}\n",
      "Valid:  [ 0/70]  eta: 0:00:31  loss: 0.1905 (0.1905)  CLS_Loss: 0.1905 (0.1905)  SEG_Loss: 0.0000 (0.0000)  time: 0.4562  data: 0.4195  max mem: 3309\n",
      "Valid:  [10/70]  eta: 0:00:04  loss: 0.1587 (0.4902)  CLS_Loss: 0.1587 (0.6208)  SEG_Loss: 0.0000 (-0.1306)  dice: 1.4587 (1.5510)  time: 0.0695  data: 0.0383  max mem: 3309\n",
      "Valid:  [20/70]  eta: 0:00:02  loss: 0.1476 (0.4775)  CLS_Loss: 0.1476 (0.5726)  SEG_Loss: 0.0000 (-0.0950)  dice: 1.4587 (1.4596)  time: 0.0297  data: 0.0002  max mem: 3309\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.1030 (0.4140)  CLS_Loss: 0.1030 (0.4884)  SEG_Loss: 0.0000 (-0.0744)  dice: 1.4062 (1.4507)  time: 0.0277  data: 0.0001  max mem: 3309\n",
      "Valid:  [40/70]  eta: 0:00:01  loss: 0.1482 (0.4721)  CLS_Loss: 0.1482 (0.5082)  SEG_Loss: 0.0000 (-0.0362)  dice: 1.3532 (1.2460)  time: 0.0270  data: 0.0001  max mem: 3309\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.1568 (0.5762)  CLS_Loss: 0.1568 (0.6218)  SEG_Loss: 0.0000 (-0.0456)  dice: 1.3532 (1.2355)  time: 0.0278  data: 0.0001  max mem: 3309\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.1424 (0.5047)  CLS_Loss: 0.1424 (0.5429)  SEG_Loss: 0.0000 (-0.0381)  dice: 1.3532 (1.2355)  time: 0.0270  data: 0.0001  max mem: 3309\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.1379 (0.4959)  CLS_Loss: 0.1379 (0.5448)  SEG_Loss: 0.0000 (-0.0490)  dice: 1.4062 (1.3008)  time: 0.0260  data: 0.0001  max mem: 3309\n",
      "Valid: Total time: 0:00:02 (0.0359 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.4958706, 'CLS_Loss': 0.5448395, 'SEG_Loss': -0.048969, 'dice': 1.3007898, 'auc': 0.4987245, 'f1': 0.0, 'acc': 0.8, 'sen': 0.0, 'spe': 1.0}\n",
      "Epoch: [4]  [ 0/33]  eta: 0:00:17  lr: 0.000100  loss: 0.8177 (0.8177)  CLS_Loss: 0.2594 (0.2594)  SEG_Loss: 0.5583 (0.5583)  time: 0.5309  data: 0.4411  max mem: 3309\n",
      "Epoch: [4]  [10/33]  eta: 0:00:03  lr: 0.000100  loss: 0.7747 (0.7685)  CLS_Loss: 0.4388 (0.4647)  SEG_Loss: 0.2389 (0.3038)  time: 0.1397  data: 0.0402  max mem: 3309\n",
      "Epoch: [4]  [20/33]  eta: 0:00:01  lr: 0.000100  loss: 0.6769 (0.6947)  CLS_Loss: 0.5064 (0.5282)  SEG_Loss: 0.0185 (0.1665)  time: 0.1010  data: 0.0001  max mem: 3309\n",
      "Epoch: [4]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.4948 (0.6572)  CLS_Loss: 0.5064 (0.5276)  SEG_Loss: 0.0000 (0.1296)  time: 0.1840  data: 0.0001  max mem: 3309\n",
      "Epoch: [4]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.4617 (0.6582)  CLS_Loss: 0.5039 (0.5276)  SEG_Loss: 0.0000 (0.1306)  time: 0.2055  data: 0.0001  max mem: 3309\n",
      "Epoch: [4] Total time: 0:00:06 (0.1824 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.6582415, 'CLS_Loss': 0.5276321, 'SEG_Loss': 0.1306094}\n",
      "Valid:  [ 0/70]  eta: 0:00:30  loss: 0.1693 (0.1693)  CLS_Loss: 0.1693 (0.1693)  SEG_Loss: 0.0000 (0.0000)  time: 0.4376  data: 0.4010  max mem: 3309\n",
      "Valid:  [10/70]  eta: 0:00:03  loss: 0.1693 (0.3736)  CLS_Loss: 0.2569 (0.5565)  SEG_Loss: 0.0000 (-0.1829)  dice: 1.4536 (1.5295)  time: 0.0661  data: 0.0366  max mem: 3309\n",
      "Valid:  [20/70]  eta: 0:00:02  loss: 0.2120 (0.3506)  CLS_Loss: 0.2569 (0.4581)  SEG_Loss: 0.0000 (-0.1075)  dice: 1.4536 (1.4758)  time: 0.0277  data: 0.0001  max mem: 3309\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.2120 (0.3556)  CLS_Loss: 0.2120 (0.4744)  SEG_Loss: 0.0000 (-0.1188)  dice: 1.7960 (1.5713)  time: 0.0264  data: 0.0001  max mem: 3309\n",
      "Valid:  [40/70]  eta: 0:00:01  loss: 0.2148 (0.4207)  CLS_Loss: 0.2148 (0.5136)  SEG_Loss: 0.0000 (-0.0929)  dice: 1.4536 (1.4597)  time: 0.0268  data: 0.0001  max mem: 3309\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.2688 (0.4871)  CLS_Loss: 0.2688 (0.5699)  SEG_Loss: 0.0000 (-0.0829)  dice: 1.3285 (1.3506)  time: 0.0276  data: 0.0001  max mem: 3309\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.3146 (0.4647)  CLS_Loss: 0.3146 (0.5340)  SEG_Loss: 0.0000 (-0.0693)  dice: 1.3285 (1.3506)  time: 0.0277  data: 0.0001  max mem: 3309\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.2570 (0.4558)  CLS_Loss: 0.2570 (0.5226)  SEG_Loss: 0.0000 (-0.0668)  dice: 1.3285 (1.3625)  time: 0.0271  data: 0.0001  max mem: 3309\n",
      "Valid: Total time: 0:00:02 (0.0357 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.4557778, 'CLS_Loss': 0.5226077, 'SEG_Loss': -0.0668299, 'dice': 1.3624622, 'auc': 0.557398, 'f1': 0.2, 'acc': 0.7714286, 'sen': 0.1428571, 'spe': 0.9285714}\n",
      "Epoch: [5]  [ 0/33]  eta: 0:00:22  lr: 0.000100  loss: 0.6834 (0.6834)  CLS_Loss: 1.0739 (1.0739)  SEG_Loss: -0.3905 (-0.3905)  time: 0.6678  data: 0.4768  max mem: 3309\n",
      "Epoch: [5]  [10/33]  eta: 0:00:04  lr: 0.000100  loss: 0.8548 (0.7298)  CLS_Loss: 0.4799 (0.5634)  SEG_Loss: 0.1700 (0.1665)  time: 0.2066  data: 0.0435  max mem: 3309\n",
      "Epoch: [5]  [20/33]  eta: 0:00:02  lr: 0.000100  loss: 0.6344 (0.7012)  CLS_Loss: 0.4525 (0.5545)  SEG_Loss: 0.0723 (0.1467)  time: 0.1330  data: 0.0001  max mem: 3309\n",
      "Epoch: [5]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.6383 (0.7363)  CLS_Loss: 0.4787 (0.5542)  SEG_Loss: 0.0000 (0.1821)  time: 0.1036  data: 0.0002  max mem: 3309\n",
      "Epoch: [5]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.6774 (0.7411)  CLS_Loss: 0.4483 (0.5487)  SEG_Loss: 0.0462 (0.1924)  time: 0.1019  data: 0.0001  max mem: 3309\n",
      "Epoch: [5] Total time: 0:00:04 (0.1431 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.7411179, 'CLS_Loss': 0.5487423, 'SEG_Loss': 0.1923756}\n",
      "Valid:  [ 0/70]  eta: 0:00:33  loss: 0.2657 (0.2657)  CLS_Loss: 0.2657 (0.2657)  SEG_Loss: 0.0000 (0.0000)  time: 0.4726  data: 0.4494  max mem: 3309\n",
      "Valid:  [10/70]  eta: 0:00:03  loss: 0.2657 (0.4588)  CLS_Loss: 0.2657 (0.4794)  SEG_Loss: 0.0000 (-0.0206)  dice: 1.0559 (1.1307)  time: 0.0597  data: 0.0411  max mem: 3309\n",
      "Valid:  [20/70]  eta: 0:00:02  loss: 0.2167 (0.3377)  CLS_Loss: 0.2167 (0.3485)  SEG_Loss: 0.0000 (-0.0108)  dice: 1.0559 (1.1307)  time: 0.0196  data: 0.0002  max mem: 3309\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.1868 (0.3578)  CLS_Loss: 0.1868 (0.4486)  SEG_Loss: 0.0000 (-0.0908)  dice: 1.3977 (1.4929)  time: 0.0261  data: 0.0002  max mem: 3309\n",
      "Valid:  [40/70]  eta: 0:00:01  loss: 0.1977 (0.3400)  CLS_Loss: 0.1977 (0.4409)  SEG_Loss: 0.0000 (-0.1008)  dice: 1.5827 (1.5500)  time: 0.0302  data: 0.0001  max mem: 3309\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.1596 (0.3283)  CLS_Loss: 0.1596 (0.4161)  SEG_Loss: 0.0000 (-0.0878)  dice: 1.5827 (1.5302)  time: 0.0294  data: 0.0001  max mem: 3309\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.2081 (0.3739)  CLS_Loss: 0.2081 (0.4524)  SEG_Loss: 0.0000 (-0.0785)  dice: 1.3977 (1.4659)  time: 0.0299  data: 0.0001  max mem: 3309\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.2553 (0.4051)  CLS_Loss: 0.2553 (0.4850)  SEG_Loss: 0.0000 (-0.0799)  dice: 1.3977 (1.4289)  time: 0.0294  data: 0.0001  max mem: 3309\n",
      "Valid: Total time: 0:00:02 (0.0356 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.4051278, 'CLS_Loss': 0.4850123, 'SEG_Loss': -0.0798845, 'dice': 1.4289263, 'auc': 0.6466837, 'f1': 0.0, 'acc': 0.8, 'sen': 0.0, 'spe': 1.0}\n",
      "Epoch: [6]  [ 0/33]  eta: 0:00:27  lr: 0.000100  loss: 0.4049 (0.4049)  CLS_Loss: 0.5621 (0.5621)  SEG_Loss: -0.1572 (-0.1572)  time: 0.8268  data: 0.5547  max mem: 3309\n",
      "Epoch: [6]  [10/33]  eta: 0:00:07  lr: 0.000100  loss: 0.4254 (0.6122)  CLS_Loss: 0.4460 (0.5256)  SEG_Loss: -0.1198 (0.0866)  time: 0.3466  data: 0.0506  max mem: 3309\n",
      "Epoch: [6]  [20/33]  eta: 0:00:03  lr: 0.000100  loss: 0.5780 (0.6068)  CLS_Loss: 0.4588 (0.5329)  SEG_Loss: 0.0000 (0.0739)  time: 0.2369  data: 0.0001  max mem: 3309\n",
      "Epoch: [6]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.5886 (0.6394)  CLS_Loss: 0.5286 (0.5505)  SEG_Loss: 0.0471 (0.0889)  time: 0.1465  data: 0.0001  max mem: 3309\n",
      "Epoch: [6]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.6292 (0.6667)  CLS_Loss: 0.4969 (0.5430)  SEG_Loss: 0.0533 (0.1237)  time: 0.1323  data: 0.0001  max mem: 3309\n",
      "Epoch: [6] Total time: 0:00:07 (0.2157 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.6666825, 'CLS_Loss': 0.5430012, 'SEG_Loss': 0.1236814}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid:  [ 0/70]  eta: 0:00:30  loss: 0.2276 (0.2276)  CLS_Loss: 0.2276 (0.2276)  SEG_Loss: 0.0000 (0.0000)  time: 0.4290  data: 0.4055  max mem: 3309\n",
      "Valid:  [10/70]  eta: 0:00:03  loss: 0.1208 (0.5122)  CLS_Loss: 0.1208 (0.5194)  SEG_Loss: 0.0000 (-0.0073)  dice: 0.5951 (1.1323)  time: 0.0561  data: 0.0371  max mem: 3309\n",
      "Valid:  [20/70]  eta: 0:00:01  loss: 0.0822 (0.3241)  CLS_Loss: 0.0822 (0.3279)  SEG_Loss: 0.0000 (-0.0038)  dice: 0.5951 (1.1323)  time: 0.0183  data: 0.0002  max mem: 3309\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.1138 (0.2964)  CLS_Loss: 0.1138 (0.3217)  SEG_Loss: 0.0000 (-0.0253)  dice: 1.6696 (1.3388)  time: 0.0179  data: 0.0001  max mem: 3309\n",
      "Valid:  [40/70]  eta: 0:00:00  loss: 0.1418 (0.2623)  CLS_Loss: 0.1418 (0.2814)  SEG_Loss: 0.0000 (-0.0191)  dice: 1.6696 (1.3388)  time: 0.0181  data: 0.0001  max mem: 3309\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.1418 (0.3119)  CLS_Loss: 0.1418 (0.3486)  SEG_Loss: 0.0000 (-0.0367)  dice: 1.2493 (1.3722)  time: 0.0183  data: 0.0001  max mem: 3309\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.1652 (0.4239)  CLS_Loss: 0.1652 (0.4752)  SEG_Loss: 0.0000 (-0.0513)  dice: 1.2493 (1.3300)  time: 0.0184  data: 0.0001  max mem: 3309\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.1781 (0.4951)  CLS_Loss: 0.1781 (0.5327)  SEG_Loss: 0.0000 (-0.0376)  dice: 1.0975 (1.2403)  time: 0.0185  data: 0.0001  max mem: 3309\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import torch\n",
    "import utils\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from metrics import *\n",
    "from losses import soft_dice_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print_freq = 10\n",
    "output_dir = './checkpoints/mtl_1/'\n",
    "device     = 'cuda'\n",
    "\n",
    "# Whole LOOP\n",
    "for epoch in range(0, 200):\n",
    "    \n",
    "    ################################################################################################\n",
    "    # Training \n",
    "    ################################################################################################\n",
    "    \n",
    "    model.train(True)\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \", n=10)\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    \n",
    "    for batch_data in metric_logger.log_every(data_loader_train, print_freq, header):\n",
    "        \n",
    "        input   = batch_data[0].to(device).float()\n",
    "        seg_gt  = batch_data[1].to(device).float()\n",
    "        cls_gt  = seg_gt.flatten(1).bool().any(dim=1).float().unsqueeze(1)\n",
    "        \n",
    "        cls_pred, seg_pred = model(input)\n",
    "\n",
    "        loss, loss_detail = criterion(cls_pred=cls_pred, seg_pred=seg_pred, cls_gt=cls_gt, seg_gt=seg_gt)\n",
    "        \n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        metric_logger.update(loss=loss_value)\n",
    "        if loss_detail is not None:\n",
    "            metric_logger.update(**loss_detail)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "    \n",
    "    train_stats = {k: round(meter.global_avg, 7) for k, meter in metric_logger.meters.items()}\n",
    "    print(\"Averaged train_stats: \", train_stats)\n",
    "    \n",
    "    ################################################################################################\n",
    "    # Validation\n",
    "    ################################################################################################\n",
    "    \n",
    "    model.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \", n=1)\n",
    "    header = 'Valid:'\n",
    "\n",
    "    total_cls_pred  = torch.tensor([])\n",
    "    total_cls_true  = torch.tensor([])\n",
    "    \n",
    "    for batch_data in metric_logger.log_every(data_loader_valid, print_freq, header):\n",
    "        \n",
    "        input   = batch_data[0].to(device).float()\n",
    "        seg_gt  = batch_data[1].to(device).float()\n",
    "        cls_gt  = seg_gt.flatten(1).bool().any(dim=1).float().unsqueeze(1)\n",
    "\n",
    "        cls_pred, seg_pred = model(input)\n",
    "\n",
    "        loss, loss_detail = criterion(cls_pred=cls_pred, seg_pred=seg_pred, cls_gt=cls_gt, seg_gt=seg_gt)\n",
    "    \n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "\n",
    "        # LOSS\n",
    "        metric_logger.update(loss=loss_value)        \n",
    "        if loss_detail is not None:\n",
    "            metric_logger.update(**loss_detail)\n",
    "\n",
    "        # Post-processing\n",
    "        cls_pred = torch.sigmoid(cls_pred)\n",
    "        seg_pred = torch.sigmoid(seg_pred)\n",
    "\n",
    "        total_cls_pred  = torch.cat([total_cls_pred, cls_pred.detach().cpu()])\n",
    "        total_cls_true  = torch.cat([total_cls_true, cls_gt.detach().cpu()])\n",
    "\n",
    "        # Metrics SEG\n",
    "        if seg_gt.any():\n",
    "            dice = soft_dice_score(output=seg_pred.round(), target=seg_gt, smooth=0.0)    # pred_seg must be round() !! \n",
    "            metric_logger.update(dice=dice.item())     \n",
    "\n",
    "    # Metric CLS\n",
    "    auc            = roc_auc_score(y_true=total_cls_true, y_score=total_cls_pred)\n",
    "    tp, fp, fn, tn = get_stats(total_cls_pred.round().long(), total_cls_true.long(), mode=\"binary\")        \n",
    "    f1             = f1_score(tp, fp, fn, tn, reduction=\"macro\")\n",
    "    acc            = accuracy(tp, fp, fn, tn, reduction=\"macro\")\n",
    "    sen            = sensitivity(tp, fp, fn, tn, reduction=\"macro\")\n",
    "    spe            = specificity(tp, fp, fn, tn, reduction=\"macro\")\n",
    "\n",
    "    metric_logger.update(auc=auc, f1=f1, acc=acc, sen=sen, spe=spe)          \n",
    "    \n",
    "    valid_stats = {k: round(meter.global_avg, 7) for k, meter in metric_logger.meters.items()}\n",
    "    print(\"Averaged valid_stats: \", valid_stats)\n",
    "    \n",
    "    ################################################################################################\n",
    "    # Save & Log\n",
    "    ################################################################################################\n",
    "    \n",
    "    checkpoint_paths = output_dir + '/epoch_' + str(epoch) + '_checkpoint.pth'\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "    }, checkpoint_paths)\n",
    "\n",
    "    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                **{f'valid_{k}': v for k, v in valid_stats.items()},\n",
    "                'epoch': epoch}\n",
    "\n",
    "    if output_dir:\n",
    "        with open(output_dir + \"/log.txt\", \"a\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_log(path):\n",
    "    log_list = []\n",
    "    lines = open(path, 'r').read().splitlines() \n",
    "    for i in range(len(lines)):\n",
    "        exec('log_list.append('+lines[i] + ')')\n",
    "    return  log_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "51, 36, 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_list = read_log(path = '/workspace/sunggu/6.Kakao/checkpoints/[Baseline]Face_Net_DAC_fold0_upsample/log.txt')\n",
    "print(log_list[0].keys())\n",
    "result_dict = {}\n",
    "\n",
    "for key in log_list[0].keys():\n",
    "    exec( \"result_dict['\"+str(key)+\"']\" + \" = [ log_list[i]['\"+str(key)+\"'] for i in range(len(log_list)) ]\")\n",
    "\n",
    "for key in result_dict.keys():\n",
    "    plt.plot(result_dict[key])\n",
    "    plt.title(key)\n",
    "    print(\"###########################################################\")\n",
    "    print(\"Argsort = \", np.argsort(result_dict[key])[:5])\n",
    "    print(\"Value   = \", [result_dict[key][i] for i in np.argsort(result_dict[key])[:5]])\n",
    "    plt.show()\n",
    "    \n",
    "    if key == 'valid_loss':\n",
    "        print(\"Valid_Loss = \", np.argsort(result_dict[key])[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log check\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_log(path):\n",
    "    log_list = []\n",
    "    lines = open(path, 'r').read().splitlines() \n",
    "    for i in range(len(lines)):\n",
    "        exec('log_list.append('+lines[i] + ')')\n",
    "    return  log_list\n",
    "\n",
    "51, 36, 35\n",
    "\n",
    "log_list = read_log(path = '/workspace/sunggu/6.Kakao/checkpoints/[Baseline]Face_Net_DAC_fold0_upsample/log.txt')\n",
    "print(log_list[0].keys())\n",
    "result_dict = {}\n",
    "\n",
    "for key in log_list[0].keys():\n",
    "    exec( \"result_dict['\"+str(key)+\"']\" + \" = [ log_list[i]['\"+str(key)+\"'] for i in range(len(log_list)) ]\")\n",
    "\n",
    "for key in result_dict.keys():\n",
    "    plt.plot(result_dict[key])\n",
    "    plt.title(key)\n",
    "    print(\"###########################################################\")\n",
    "    print(\"Argsort = \", np.argsort(result_dict[key])[:5])\n",
    "    print(\"Value   = \", [result_dict[key][i] for i in np.argsort(result_dict[key])[:5]])\n",
    "    plt.show()\n",
    "    \n",
    "    if key == 'valid_loss':\n",
    "        print(\"Valid_Loss = \", np.argsort(result_dict[key])[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "\n",
    "print(\"Loading... Resume\")\n",
    "checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])        \n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])        \n",
    "args.start_epoch = checkpoint['epoch'] + 1  \n",
    "try:\n",
    "    log_path = os.path.dirname(args.resume)+'/log.txt'\n",
    "    lines    = open(log_path,'r').readlines()\n",
    "    val_loss_list = []\n",
    "    for l in lines:\n",
    "        exec('log_dict='+l.replace('NaN', '0'))\n",
    "        val_loss_list.append(log_dict['valid_loss'])\n",
    "    print(\"Epoch: \", np.argmin(val_loss_list), \" Minimum Val Loss ==> \", np.min(val_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "STL_cls.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
