{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t85Nx2fQEF_s",
    "outputId": "09968e41-cfb5-47bd-8e6d-4a4a85cf2bbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 16 19:28:56 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:18:00.0 Off |                  Off |\n",
      "| 55%   79C    P2   107W / 300W |   4378MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    Off  | 00000000:3B:00.0 Off |                  Off |\n",
      "|100%   88C    P2   247W / 300W |  45210MiB / 48685MiB |     98%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    Off  | 00000000:86:00.0 Off |                  Off |\n",
      "| 60%   74C    P8    46W / 300W |      3MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000    Off  | 00000000:AF:00.0 Off |                  Off |\n",
      "| 68%   85C    P2   251W / 300W |  25408MiB / 48685MiB |     38%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "import cv2\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P: 75 (22.73%) N: 255 (77.27%) Total: 330\n",
    "P: 14 (20.00%) N: 56  (80.00%) Total: 70\n",
    "P: 21 (21.00%) N: 79  (79.00%) Total: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/sunggu/7.KOHI/Multi_task_learning_tutorials'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CN3SCwTVEF_v"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = './SSIM_seg/'\n",
    "\n",
    "x_train_list = glob.glob(os.path.join(DATA_DIR, 'train/*'))\n",
    "y_train_list = glob.glob(os.path.join(DATA_DIR, 'trainannot/*'))\n",
    "\n",
    "x_valid_list = glob.glob(os.path.join(DATA_DIR, 'val/*'))\n",
    "y_valid_list = glob.glob(os.path.join(DATA_DIR, 'valannot/*'))\n",
    "\n",
    "x_test_list  = glob.glob(os.path.join(DATA_DIR, 'test/*'))\n",
    "y_test_list  = glob.glob(os.path.join(DATA_DIR, 'testannot/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./SSIM_seg/train/1.2.276.0.7230010.3.1.4.8323329.300.1517875162.258081.png'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./SSIM_seg/val/1.2.276.0.7230010.3.1.4.8323329.1173.1517875166.626582.png'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "f0iPTFh_EF_w"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Dataset Class\n",
    "class Dataset(BaseDataset):    \n",
    "    def __init__(self, images_list, labels_list, transform):\n",
    "        self.images_list  = images_list\n",
    "        self.labels_list  = labels_list\n",
    "        self.transform    = transform\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # read data\n",
    "        image = cv2.imread(self.images_list[i], cv2.IMREAD_GRAYSCALE)\n",
    "        mask  = cv2.imread(self.labels_list[i], cv2.IMREAD_GRAYSCALE)\n",
    "        path  = self.images_list[i]\n",
    "        \n",
    "        mask  = np.expand_dims(mask, axis=0)\n",
    "            \n",
    "        # apply transform\n",
    "        sample = self.transform(image=image, mask=mask)\n",
    "        image, mask = sample['image'], sample['mask']        \n",
    "        \n",
    "        return image, mask, path\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     12,
     35
    ]
   },
   "outputs": [],
   "source": [
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import albumentations as albu\n",
    "\n",
    "\n",
    "def minmax_normalize(image, **kwargs):\n",
    "    if len(np.unique(image)) != 1:  # Sometimes it cause the nan inputs...\n",
    "        image = image.astype('float32')\n",
    "        image -= image.min()\n",
    "        image /= image.max() \n",
    "    return image\n",
    "\n",
    "\n",
    "train_transform = albu.Compose([\n",
    "    albu.HorizontalFlip(p=0.5),\n",
    "    albu.ShiftScaleRotate(scale_limit=0.10, shift_limit=0.10, rotate_limit=15, p=0.5),\n",
    "    albu.GaussNoise(p=0.2),\n",
    "    albu.OneOf(\n",
    "        [\n",
    "            albu.CLAHE(p=1),\n",
    "            albu.RandomBrightnessContrast(p=1),\n",
    "            albu.RandomGamma(p=1),\n",
    "        ],\n",
    "        p=0.3,\n",
    "    ),\n",
    "    albu.OneOf(\n",
    "        [\n",
    "            albu.Blur(blur_limit=3, p=1),\n",
    "            albu.MotionBlur(blur_limit=3, p=1),\n",
    "        ],\n",
    "        p=0.3,\n",
    "    ),\n",
    "    albu.Lambda(image=minmax_normalize, always_apply=True),\n",
    "    ToTensorV2(),    \n",
    "])\n",
    "\n",
    "valid_transform = albu.Compose([        \n",
    "    albu.Lambda(image=minmax_normalize, always_apply=True),\n",
    "    ToTensorV2(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset(images_list=x_train_list, labels_list=y_train_list, transform=train_transform)\n",
    "dataset_valid = Dataset(images_list=x_valid_list, labels_list=y_valid_list, transform=valid_transform)\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=10, num_workers=4, shuffle=True, pin_memory=True, drop_last=True)\n",
    "data_loader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=1,  num_workers=4, shuffle=True, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "cdMO4sJyEF_z",
    "outputId": "503b9008-e50a-41e5-bf09-ce4576d83d5f"
   },
   "outputs": [],
   "source": [
    "# # same image with different random transforms\n",
    "\n",
    "# batch = next(iter(train_loader))\n",
    "# x = batch['x'][0]\n",
    "# y_seg = batch['y_seg'][0]\n",
    "# y_cls = batch['y_cls'][0]\n",
    "\n",
    "# print(x.shape,y_seg.shape,y_cls.shape)\n",
    "# print(torch.unique(y_seg),y_cls)\n",
    "# visualize(image=x, mask=y_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Learnable Params: 14322198\n"
     ]
    }
   ],
   "source": [
    "from arch.smart_net import *\n",
    "from losses import MTL_Loss\n",
    "\n",
    "# Model\n",
    "model        = STL_2_Net(encoder_name='resnet18').to('cuda')     \n",
    "\n",
    "# Loss\n",
    "criterion    = MTL_Loss(name='STL_SEG')\n",
    "\n",
    "# Optimizer & LR Schedule   \n",
    "optimizer    = torch.optim.AdamW(params=model.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-4, amsgrad=False)\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Number of Learnable Params:', n_parameters)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "id": "ojQz_ZNgEF_0",
    "outputId": "ca6fe927-248f-42af-e694-b2b740f286e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/33]  eta: 0:00:26  lr: 0.000100  loss: 0.3585 (0.3585)  SEG_Loss: 0.3585 (0.3585)  time: 0.7908  data: 0.5204  max mem: 2750\n",
      "Epoch: [0]  [10/33]  eta: 0:00:04  lr: 0.000100  loss: 0.4467 (0.4196)  SEG_Loss: 0.4467 (0.4196)  time: 0.2016  data: 0.0475  max mem: 2917\n",
      "Epoch: [0]  [20/33]  eta: 0:00:02  lr: 0.000100  loss: 0.2561 (0.3276)  SEG_Loss: 0.2561 (0.3276)  time: 0.1330  data: 0.0002  max mem: 2917\n",
      "Epoch: [0]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.1983 (0.3209)  SEG_Loss: 0.1983 (0.3209)  time: 0.1124  data: 0.0001  max mem: 2917\n",
      "Epoch: [0]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.2331 (0.3298)  SEG_Loss: 0.2331 (0.3298)  time: 0.1099  data: 0.0001  max mem: 2917\n",
      "Epoch: [0] Total time: 0:00:04 (0.1463 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.32983, 'SEG_Loss': 0.32983}\n",
      "Valid:  [ 0/70]  eta: 0:00:24  loss: 0.0000 (0.0000)  SEG_Loss: 0.0000 (0.0000)  time: 0.3543  data: 0.3229  max mem: 2917\n",
      "Valid:  [10/70]  eta: 0:00:02  loss: 0.0000 (0.0818)  SEG_Loss: 0.0000 (0.0818)  dice: 0.0498 (0.0611)  time: 0.0493  data: 0.0295  max mem: 2917\n",
      "Valid:  [20/70]  eta: 0:00:01  loss: 0.0000 (0.0428)  SEG_Loss: 0.0000 (0.0428)  dice: 0.0498 (0.0611)  time: 0.0172  data: 0.0002  max mem: 2917\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.0000 (0.0763)  SEG_Loss: 0.0000 (0.0763)  dice: 0.0498 (0.2778)  time: 0.0154  data: 0.0001  max mem: 2917\n",
      "Valid:  [40/70]  eta: 0:00:00  loss: 0.0000 (0.0601)  SEG_Loss: 0.0000 (0.0601)  dice: 0.0723 (0.3300)  time: 0.0141  data: 0.0001  max mem: 2917\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.0000 (0.0645)  SEG_Loss: 0.0000 (0.0645)  dice: 0.5657 (0.4973)  time: 0.0133  data: 0.0001  max mem: 2917\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.0000 (0.0610)  SEG_Loss: 0.0000 (0.0610)  dice: 0.6434 (0.5714)  time: 0.0134  data: 0.0001  max mem: 2917\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.0000 (0.0565)  SEG_Loss: 0.0000 (0.0565)  dice: 0.5657 (0.5541)  time: 0.0132  data: 0.0001  max mem: 2917\n",
      "Valid: Total time: 0:00:01 (0.0220 s / it)\n",
      "Averaged valid_stats:  {'loss': 0.0565363, 'SEG_Loss': 0.0565363, 'dice': 0.5541178}\n",
      "Epoch: [1]  [ 0/33]  eta: 0:00:22  lr: 0.000100  loss: -0.0753 (-0.0753)  SEG_Loss: -0.0753 (-0.0753)  time: 0.6704  data: 0.4924  max mem: 3302\n",
      "Epoch: [1]  [10/33]  eta: 0:00:04  lr: 0.000100  loss: 0.2841 (0.2132)  SEG_Loss: 0.2841 (0.2132)  time: 0.1747  data: 0.0449  max mem: 3302\n",
      "Epoch: [1]  [20/33]  eta: 0:00:01  lr: 0.000100  loss: 0.3462 (0.2841)  SEG_Loss: 0.3462 (0.2841)  time: 0.1206  data: 0.0002  max mem: 3302\n",
      "Epoch: [1]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.1713 (0.2594)  SEG_Loss: 0.1713 (0.2594)  time: 0.1120  data: 0.0002  max mem: 3302\n",
      "Epoch: [1]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.2487 (0.2759)  SEG_Loss: 0.2487 (0.2759)  time: 0.1093  data: 0.0002  max mem: 3302\n",
      "Epoch: [1] Total time: 0:00:04 (0.1379 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.2759273, 'SEG_Loss': 0.2759273}\n",
      "Valid:  [ 0/70]  eta: 0:00:25  loss: 0.0000 (0.0000)  SEG_Loss: 0.0000 (0.0000)  time: 0.3579  data: 0.3372  max mem: 3302\n",
      "Valid:  [10/70]  eta: 0:00:02  loss: 0.0000 (0.0055)  SEG_Loss: 0.0000 (0.0055)  dice: 1.0919 (1.0919)  time: 0.0422  data: 0.0308  max mem: 3302\n",
      "Valid:  [20/70]  eta: 0:00:01  loss: 0.0000 (-0.0302)  SEG_Loss: 0.0000 (-0.0302)  dice: 1.5379 (1.3937)  time: 0.0106  data: 0.0002  max mem: 3302\n",
      "Valid:  [30/70]  eta: 0:00:00  loss: 0.0000 (-0.0152)  SEG_Loss: 0.0000 (-0.0152)  dice: 1.0919 (1.2824)  time: 0.0132  data: 0.0002  max mem: 3302\n",
      "Valid:  [40/70]  eta: 0:00:00  loss: 0.0000 (-0.0163)  SEG_Loss: 0.0000 (-0.0163)  dice: 1.0919 (1.3006)  time: 0.0160  data: 0.0001  max mem: 3302\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.0000 (-0.0174)  SEG_Loss: 0.0000 (-0.0174)  dice: 1.2868 (1.2310)  time: 0.0164  data: 0.0001  max mem: 3302\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.0000 (-0.0165)  SEG_Loss: 0.0000 (-0.0165)  dice: 1.2868 (1.2604)  time: 0.0168  data: 0.0002  max mem: 3302\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.0000 (-0.0092)  SEG_Loss: 0.0000 (-0.0092)  dice: 1.2868 (1.2177)  time: 0.0165  data: 0.0002  max mem: 3302\n",
      "Valid: Total time: 0:00:01 (0.0213 s / it)\n",
      "Averaged valid_stats:  {'loss': -0.0091667, 'SEG_Loss': -0.0091667, 'dice': 1.2177284}\n",
      "Epoch: [2]  [ 0/33]  eta: 0:00:20  lr: 0.000100  loss: 0.7817 (0.7817)  SEG_Loss: 0.7817 (0.7817)  time: 0.6206  data: 0.4394  max mem: 3304\n",
      "Epoch: [2]  [10/33]  eta: 0:00:04  lr: 0.000100  loss: 0.1386 (0.2036)  SEG_Loss: 0.1386 (0.2036)  time: 0.1968  data: 0.0402  max mem: 3304\n",
      "Epoch: [2]  [20/33]  eta: 0:00:02  lr: 0.000100  loss: 0.1329 (0.2031)  SEG_Loss: 0.1329 (0.2031)  time: 0.1744  data: 0.0002  max mem: 3304\n",
      "Epoch: [2]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.2028 (0.2252)  SEG_Loss: 0.2028 (0.2252)  time: 0.2224  data: 0.0002  max mem: 3304\n",
      "Epoch: [2]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.1944 (0.2341)  SEG_Loss: 0.1944 (0.2341)  time: 0.2404  data: 0.0002  max mem: 3304\n",
      "Epoch: [2] Total time: 0:00:07 (0.2229 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.2341398, 'SEG_Loss': 0.2341398}\n",
      "Valid:  [ 0/70]  eta: 0:00:25  loss: 0.0000 (0.0000)  SEG_Loss: 0.0000 (0.0000)  time: 0.3688  data: 0.3427  max mem: 3304\n",
      "Valid:  [10/70]  eta: 0:00:03  loss: 0.0000 (-0.1001)  SEG_Loss: 0.0000 (-0.1001)  dice: 1.4166 (1.5134)  time: 0.0527  data: 0.0313  max mem: 3304\n",
      "Valid:  [20/70]  eta: 0:00:01  loss: 0.0000 (-0.0417)  SEG_Loss: 0.0000 (-0.0417)  dice: 1.3016 (1.4053)  time: 0.0197  data: 0.0001  max mem: 3304\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.0000 (-0.0210)  SEG_Loss: 0.0000 (-0.0210)  dice: 1.3016 (1.2383)  time: 0.0187  data: 0.0001  max mem: 3304\n",
      "Valid:  [40/70]  eta: 0:00:00  loss: 0.0000 (-0.0459)  SEG_Loss: 0.0000 (-0.0459)  dice: 1.4166 (1.3480)  time: 0.0185  data: 0.0001  max mem: 3304\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.0000 (-0.0369)  SEG_Loss: 0.0000 (-0.0369)  dice: 1.4166 (1.3480)  time: 0.0190  data: 0.0001  max mem: 3304\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.0000 (-0.0313)  SEG_Loss: 0.0000 (-0.0313)  dice: 1.4166 (1.3570)  time: 0.0208  data: 0.0001  max mem: 3304\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.0000 (-0.0258)  SEG_Loss: 0.0000 (-0.0258)  dice: 1.3016 (1.2781)  time: 0.0212  data: 0.0001  max mem: 3304\n",
      "Valid: Total time: 0:00:01 (0.0276 s / it)\n",
      "Averaged valid_stats:  {'loss': -0.025825, 'SEG_Loss': -0.025825, 'dice': 1.2780843}\n",
      "Epoch: [3]  [ 0/33]  eta: 0:00:24  lr: 0.000100  loss: 0.7104 (0.7104)  SEG_Loss: 0.7104 (0.7104)  time: 0.7509  data: 0.5456  max mem: 3304\n",
      "Epoch: [3]  [10/33]  eta: 0:00:05  lr: 0.000100  loss: 0.1559 (0.2318)  SEG_Loss: 0.1559 (0.2318)  time: 0.2233  data: 0.0498  max mem: 3304\n",
      "Epoch: [3]  [20/33]  eta: 0:00:02  lr: 0.000100  loss: 0.1559 (0.2354)  SEG_Loss: 0.1559 (0.2354)  time: 0.1446  data: 0.0002  max mem: 3304\n",
      "Epoch: [3]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.1110 (0.2176)  SEG_Loss: 0.1110 (0.2176)  time: 0.1103  data: 0.0002  max mem: 3304\n",
      "Epoch: [3]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.2061 (0.2204)  SEG_Loss: 0.2061 (0.2204)  time: 0.1075  data: 0.0002  max mem: 3304\n",
      "Epoch: [3] Total time: 0:00:05 (0.1521 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.2203858, 'SEG_Loss': 0.2203858}\n",
      "Valid:  [ 0/70]  eta: 0:00:26  loss: 0.0000 (0.0000)  SEG_Loss: 0.0000 (0.0000)  time: 0.3718  data: 0.3514  max mem: 3304\n",
      "Valid:  [10/70]  eta: 0:00:02  loss: 0.0000 (-0.1328)  SEG_Loss: 0.0000 (-0.1328)  dice: 1.4758 (1.5216)  time: 0.0476  data: 0.0321  max mem: 3304\n",
      "Valid:  [20/70]  eta: 0:00:01  loss: 0.0000 (-0.0957)  SEG_Loss: 0.0000 (-0.0957)  dice: 1.3056 (1.3529)  time: 0.0157  data: 0.0001  max mem: 3304\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.0000 (-0.0777)  SEG_Loss: 0.0000 (-0.0777)  dice: 1.3913 (1.3583)  time: 0.0155  data: 0.0001  max mem: 3304\n",
      "Valid:  [40/70]  eta: 0:00:00  loss: 0.0000 (-0.0915)  SEG_Loss: 0.0000 (-0.0915)  dice: 1.4758 (1.4374)  time: 0.0149  data: 0.0001  max mem: 3304\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.0000 (-0.0607)  SEG_Loss: 0.0000 (-0.0607)  dice: 1.3913 (1.2699)  time: 0.0152  data: 0.0001  max mem: 3304\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.0000 (-0.0508)  SEG_Loss: 0.0000 (-0.0508)  dice: 1.3913 (1.2699)  time: 0.0145  data: 0.0001  max mem: 3304\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.0000 (-0.0512)  SEG_Loss: 0.0000 (-0.0512)  dice: 1.3056 (1.2651)  time: 0.0139  data: 0.0001  max mem: 3304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Total time: 0:00:01 (0.0221 s / it)\n",
      "Averaged valid_stats:  {'loss': -0.0512388, 'SEG_Loss': -0.0512388, 'dice': 1.265149}\n",
      "Epoch: [4]  [ 0/33]  eta: 0:00:24  lr: 0.000100  loss: -0.4186 (-0.4186)  SEG_Loss: -0.4186 (-0.4186)  time: 0.7299  data: 0.4678  max mem: 3308\n",
      "Epoch: [4]  [10/33]  eta: 0:00:07  lr: 0.000100  loss: -0.0439 (0.1165)  SEG_Loss: -0.0439 (0.1165)  time: 0.3336  data: 0.0427  max mem: 3308\n",
      "Epoch: [4]  [20/33]  eta: 0:00:03  lr: 0.000100  loss: 0.0000 (0.1233)  SEG_Loss: 0.0000 (0.1233)  time: 0.2645  data: 0.0001  max mem: 3308\n",
      "Epoch: [4]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.0944 (0.1781)  SEG_Loss: 0.0944 (0.1781)  time: 0.1864  data: 0.0001  max mem: 3308\n",
      "Epoch: [4]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.0944 (0.1588)  SEG_Loss: 0.0944 (0.1588)  time: 0.1774  data: 0.0001  max mem: 3308\n",
      "Epoch: [4] Total time: 0:00:07 (0.2371 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.1588362, 'SEG_Loss': 0.1588362}\n",
      "Valid:  [ 0/70]  eta: 0:00:25  loss: 0.0069 (0.0069)  SEG_Loss: 0.0069 (0.0069)  dice: 1.0066 (1.0066)  time: 0.3592  data: 0.3417  max mem: 3308\n",
      "Valid:  [10/70]  eta: 0:00:02  loss: 0.0000 (-0.1425)  SEG_Loss: 0.0000 (-0.1425)  dice: 1.8033 (1.5541)  time: 0.0423  data: 0.0312  max mem: 3308\n",
      "Valid:  [20/70]  eta: 0:00:01  loss: 0.0000 (-0.0771)  SEG_Loss: 0.0000 (-0.0771)  dice: 1.0334 (1.4239)  time: 0.0103  data: 0.0001  max mem: 3308\n",
      "Valid:  [30/70]  eta: 0:00:00  loss: 0.0000 (-0.0919)  SEG_Loss: 0.0000 (-0.0919)  dice: 1.1753 (1.4140)  time: 0.0099  data: 0.0001  max mem: 3308\n",
      "Valid:  [40/70]  eta: 0:00:00  loss: 0.0000 (-0.0976)  SEG_Loss: 0.0000 (-0.0976)  dice: 1.3679 (1.4361)  time: 0.0098  data: 0.0001  max mem: 3308\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.0000 (-0.0904)  SEG_Loss: 0.0000 (-0.0904)  dice: 1.3679 (1.4514)  time: 0.0097  data: 0.0001  max mem: 3308\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.0000 (-0.0659)  SEG_Loss: 0.0000 (-0.0659)  dice: 1.3679 (1.3727)  time: 0.0097  data: 0.0001  max mem: 3308\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.0000 (-0.0621)  SEG_Loss: 0.0000 (-0.0621)  dice: 1.3679 (1.3756)  time: 0.0094  data: 0.0001  max mem: 3308\n",
      "Valid: Total time: 0:00:01 (0.0165 s / it)\n",
      "Averaged valid_stats:  {'loss': -0.0621337, 'SEG_Loss': -0.0621337, 'dice': 1.3755766}\n",
      "Epoch: [5]  [ 0/33]  eta: 0:00:22  lr: 0.000100  loss: 0.0245 (0.0245)  SEG_Loss: 0.0245 (0.0245)  time: 0.6964  data: 0.4346  max mem: 3308\n",
      "Epoch: [5]  [10/33]  eta: 0:00:07  lr: 0.000100  loss: 0.4163 (0.2817)  SEG_Loss: 0.4163 (0.2817)  time: 0.3462  data: 0.0397  max mem: 3308\n",
      "Epoch: [5]  [20/33]  eta: 0:00:03  lr: 0.000100  loss: 0.3521 (0.3086)  SEG_Loss: 0.3521 (0.3086)  time: 0.2803  data: 0.0001  max mem: 3308\n",
      "Epoch: [5]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.0969 (0.2093)  SEG_Loss: 0.0969 (0.2093)  time: 0.2309  data: 0.0001  max mem: 3308\n",
      "Epoch: [5]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.0466 (0.1753)  SEG_Loss: 0.0466 (0.1753)  time: 0.2096  data: 0.0001  max mem: 3308\n",
      "Epoch: [5] Total time: 0:00:08 (0.2658 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.1752533, 'SEG_Loss': 0.1752533}\n",
      "Valid:  [ 0/70]  eta: 0:00:27  loss: 0.0000 (0.0000)  SEG_Loss: 0.0000 (0.0000)  time: 0.3928  data: 0.3736  max mem: 3308\n",
      "Valid:  [10/70]  eta: 0:00:02  loss: 0.0000 (-0.1244)  SEG_Loss: 0.0000 (-0.1244)  dice: 1.5994 (1.7264)  time: 0.0465  data: 0.0341  max mem: 3308\n",
      "Valid:  [20/70]  eta: 0:00:01  loss: 0.0000 (-0.1232)  SEG_Loss: 0.0000 (-0.1232)  dice: 1.5994 (1.6021)  time: 0.0119  data: 0.0002  max mem: 3308\n",
      "Valid:  [30/70]  eta: 0:00:00  loss: 0.0000 (-0.0883)  SEG_Loss: 0.0000 (-0.0883)  dice: 1.4953 (1.5272)  time: 0.0120  data: 0.0002  max mem: 3308\n",
      "Valid:  [40/70]  eta: 0:00:00  loss: 0.0000 (-0.0429)  SEG_Loss: 0.0000 (-0.0429)  dice: 1.3685 (1.2641)  time: 0.0114  data: 0.0002  max mem: 3308\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.0000 (-0.0312)  SEG_Loss: 0.0000 (-0.0312)  dice: 1.3685 (1.2494)  time: 0.0106  data: 0.0001  max mem: 3308\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.0000 (-0.0367)  SEG_Loss: 0.0000 (-0.0367)  dice: 1.3685 (1.2652)  time: 0.0105  data: 0.0001  max mem: 3308\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.0000 (-0.0469)  SEG_Loss: 0.0000 (-0.0469)  dice: 1.3321 (1.2942)  time: 0.0101  data: 0.0001  max mem: 3308\n",
      "Valid: Total time: 0:00:01 (0.0183 s / it)\n",
      "Averaged valid_stats:  {'loss': -0.0468597, 'SEG_Loss': -0.0468597, 'dice': 1.2941913}\n",
      "Epoch: [6]  [ 0/33]  eta: 0:00:22  lr: 0.000100  loss: 0.6902 (0.6902)  SEG_Loss: 0.6902 (0.6902)  time: 0.6754  data: 0.4122  max mem: 3308\n",
      "Epoch: [6]  [10/33]  eta: 0:00:07  lr: 0.000100  loss: 0.2010 (0.2116)  SEG_Loss: 0.2010 (0.2116)  time: 0.3440  data: 0.0376  max mem: 3308\n",
      "Epoch: [6]  [20/33]  eta: 0:00:04  lr: 0.000100  loss: 0.2010 (0.2272)  SEG_Loss: 0.2010 (0.2272)  time: 0.2934  data: 0.0002  max mem: 3308\n",
      "Epoch: [6]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.1089 (0.1700)  SEG_Loss: 0.1089 (0.1700)  time: 0.2524  data: 0.0002  max mem: 3308\n",
      "Epoch: [6]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.1089 (0.1744)  SEG_Loss: 0.1089 (0.1744)  time: 0.2312  data: 0.0002  max mem: 3308\n",
      "Epoch: [6] Total time: 0:00:09 (0.2796 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.1744313, 'SEG_Loss': 0.1744313}\n",
      "Valid:  [ 0/70]  eta: 0:00:23  loss: 0.0000 (0.0000)  SEG_Loss: 0.0000 (0.0000)  time: 0.3353  data: 0.3169  max mem: 3308\n",
      "Valid:  [10/70]  eta: 0:00:02  loss: 0.0000 (0.0018)  SEG_Loss: 0.0000 (0.0018)  dice: 0.9865 (0.9865)  time: 0.0411  data: 0.0290  max mem: 3308\n",
      "Valid:  [20/70]  eta: 0:00:01  loss: 0.0000 (-0.0275)  SEG_Loss: 0.0000 (-0.0275)  dice: 0.9865 (1.3036)  time: 0.0111  data: 0.0002  max mem: 3308\n",
      "Valid:  [30/70]  eta: 0:00:00  loss: 0.0000 (-0.0349)  SEG_Loss: 0.0000 (-0.0349)  dice: 1.1835 (1.2212)  time: 0.0105  data: 0.0001  max mem: 3308\n",
      "Valid:  [40/70]  eta: 0:00:00  loss: 0.0000 (-0.0404)  SEG_Loss: 0.0000 (-0.0404)  dice: 1.2683 (1.3087)  time: 0.0104  data: 0.0001  max mem: 3308\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.0000 (-0.0712)  SEG_Loss: 0.0000 (-0.0712)  dice: 1.5128 (1.4034)  time: 0.0114  data: 0.0001  max mem: 3308\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.0000 (-0.0598)  SEG_Loss: 0.0000 (-0.0598)  dice: 1.2683 (1.3700)  time: 0.0121  data: 0.0001  max mem: 3308\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.0000 (-0.0627)  SEG_Loss: 0.0000 (-0.0627)  dice: 1.3385 (1.3720)  time: 0.0115  data: 0.0001  max mem: 3308\n",
      "Valid: Total time: 0:00:01 (0.0174 s / it)\n",
      "Averaged valid_stats:  {'loss': -0.0626875, 'SEG_Loss': -0.0626875, 'dice': 1.3720372}\n",
      "Epoch: [7]  [ 0/33]  eta: 0:00:22  lr: 0.000100  loss: 0.3584 (0.3584)  SEG_Loss: 0.3584 (0.3584)  time: 0.6735  data: 0.4144  max mem: 3308\n",
      "Epoch: [7]  [10/33]  eta: 0:00:07  lr: 0.000100  loss: 0.0000 (0.0251)  SEG_Loss: 0.0000 (0.0251)  time: 0.3211  data: 0.0378  max mem: 3308\n",
      "Epoch: [7]  [20/33]  eta: 0:00:03  lr: 0.000100  loss: 0.0000 (0.0791)  SEG_Loss: 0.0000 (0.0791)  time: 0.2605  data: 0.0001  max mem: 3308\n",
      "Epoch: [7]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.0235 (0.1189)  SEG_Loss: 0.0235 (0.1189)  time: 0.2173  data: 0.0001  max mem: 3308\n",
      "Epoch: [7]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.2783 (0.1299)  SEG_Loss: 0.2783 (0.1299)  time: 0.2112  data: 0.0001  max mem: 3308\n",
      "Epoch: [7] Total time: 0:00:08 (0.2535 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.1299435, 'SEG_Loss': 0.1299435}\n",
      "Valid:  [ 0/70]  eta: 0:00:28  loss: 0.0000 (0.0000)  SEG_Loss: 0.0000 (0.0000)  time: 0.4053  data: 0.3841  max mem: 3308\n",
      "Valid:  [10/70]  eta: 0:00:03  loss: 0.0000 (-0.0931)  SEG_Loss: 0.0000 (-0.0931)  dice: 1.3092 (1.3860)  time: 0.0508  data: 0.0350  max mem: 3308\n",
      "Valid:  [20/70]  eta: 0:00:01  loss: 0.0000 (-0.0862)  SEG_Loss: 0.0000 (-0.0862)  dice: 1.3092 (1.4922)  time: 0.0150  data: 0.0001  max mem: 3308\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.0000 (-0.0881)  SEG_Loss: 0.0000 (-0.0881)  dice: 1.3465 (1.4881)  time: 0.0151  data: 0.0001  max mem: 3308\n",
      "Valid:  [40/70]  eta: 0:00:00  loss: 0.0000 (-0.1058)  SEG_Loss: 0.0000 (-0.1058)  dice: 1.6134 (1.5820)  time: 0.0158  data: 0.0001  max mem: 3308\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.0000 (-0.0851)  SEG_Loss: 0.0000 (-0.0851)  dice: 1.6134 (1.5820)  time: 0.0159  data: 0.0001  max mem: 3308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.0000 (-0.0612)  SEG_Loss: 0.0000 (-0.0612)  dice: 1.3465 (1.4099)  time: 0.0155  data: 0.0001  max mem: 3308\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.0000 (-0.0658)  SEG_Loss: 0.0000 (-0.0658)  dice: 1.3092 (1.3601)  time: 0.0147  data: 0.0001  max mem: 3308\n",
      "Valid: Total time: 0:00:01 (0.0233 s / it)\n",
      "Averaged valid_stats:  {'loss': -0.0657751, 'SEG_Loss': -0.0657751, 'dice': 1.360118}\n",
      "Epoch: [8]  [ 0/33]  eta: 0:00:21  lr: 0.000100  loss: -0.5948 (-0.5948)  SEG_Loss: -0.5948 (-0.5948)  time: 0.6465  data: 0.4413  max mem: 3308\n",
      "Epoch: [8]  [10/33]  eta: 0:00:06  lr: 0.000100  loss: -0.0006 (0.0241)  SEG_Loss: -0.0006 (0.0241)  time: 0.2621  data: 0.0403  max mem: 3308\n",
      "Epoch: [8]  [20/33]  eta: 0:00:03  lr: 0.000100  loss: -0.0006 (0.0503)  SEG_Loss: -0.0006 (0.0503)  time: 0.2258  data: 0.0002  max mem: 3308\n",
      "Epoch: [8]  [30/33]  eta: 0:00:00  lr: 0.000100  loss: 0.0437 (0.1148)  SEG_Loss: 0.0437 (0.1148)  time: 0.2571  data: 0.0001  max mem: 3308\n",
      "Epoch: [8]  [32/33]  eta: 0:00:00  lr: 0.000100  loss: 0.0437 (0.1242)  SEG_Loss: 0.0437 (0.1242)  time: 0.2607  data: 0.0001  max mem: 3308\n",
      "Epoch: [8] Total time: 0:00:08 (0.2619 s / it)\n",
      "Averaged train_stats:  {'lr': 0.0001, 'loss': 0.1241737, 'SEG_Loss': 0.1241737}\n",
      "Valid:  [ 0/70]  eta: 0:00:29  loss: -0.0445 (-0.0445)  SEG_Loss: -0.0445 (-0.0445)  dice: 1.0138 (1.0138)  time: 0.4221  data: 0.3992  max mem: 3308\n",
      "Valid:  [10/70]  eta: 0:00:03  loss: 0.0000 (-0.0040)  SEG_Loss: 0.0000 (-0.0040)  dice: 1.0138 (1.0138)  time: 0.0506  data: 0.0364  max mem: 3308\n",
      "Valid:  [20/70]  eta: 0:00:01  loss: 0.0000 (-0.1120)  SEG_Loss: 0.0000 (-0.1120)  dice: 1.6097 (1.5046)  time: 0.0136  data: 0.0001  max mem: 3308\n",
      "Valid:  [30/70]  eta: 0:00:01  loss: 0.0000 (-0.0880)  SEG_Loss: 0.0000 (-0.0880)  dice: 1.3835 (1.4234)  time: 0.0136  data: 0.0001  max mem: 3308\n",
      "Valid:  [40/70]  eta: 0:00:00  loss: 0.0000 (-0.0805)  SEG_Loss: 0.0000 (-0.0805)  dice: 1.3835 (1.3939)  time: 0.0135  data: 0.0001  max mem: 3308\n",
      "Valid:  [50/70]  eta: 0:00:00  loss: 0.0000 (-0.0647)  SEG_Loss: 0.0000 (-0.0647)  dice: 1.3835 (1.3939)  time: 0.0132  data: 0.0001  max mem: 3308\n",
      "Valid:  [60/70]  eta: 0:00:00  loss: 0.0000 (-0.0768)  SEG_Loss: 0.0000 (-0.0768)  dice: 1.3835 (1.3923)  time: 0.0134  data: 0.0001  max mem: 3308\n",
      "Valid:  [69/70]  eta: 0:00:00  loss: 0.0000 (-0.0707)  SEG_Loss: 0.0000 (-0.0707)  dice: 1.3637 (1.3846)  time: 0.0137  data: 0.0001  max mem: 3308\n",
      "Valid: Total time: 0:00:01 (0.0217 s / it)\n",
      "Averaged valid_stats:  {'loss': -0.0706524, 'SEG_Loss': -0.0706524, 'dice': 1.3846298}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import torch\n",
    "import utils\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from metrics import *\n",
    "from losses import soft_dice_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print_freq = 10\n",
    "output_dir = './checkpoints/stl_seg/'\n",
    "device     = 'cuda'\n",
    "\n",
    "# Whole LOOP\n",
    "for epoch in range(0, 200):\n",
    "    \n",
    "    ################################################################################################\n",
    "    # Training \n",
    "    ################################################################################################\n",
    "    \n",
    "    model.train(True)\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \", n=10)\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    \n",
    "    for batch_data in metric_logger.log_every(data_loader_train, print_freq, header):\n",
    "        \n",
    "        input   = batch_data[0].to(device).float()\n",
    "        seg_gt  = batch_data[1].to(device).float()\n",
    "\n",
    "        seg_pred = model(input)\n",
    "\n",
    "        loss, loss_detail = criterion(seg_pred=seg_pred, seg_gt=seg_gt)\n",
    "        \n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        metric_logger.update(loss=loss_value)\n",
    "        if loss_detail is not None:\n",
    "            metric_logger.update(**loss_detail)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "    \n",
    "    train_stats = {k: round(meter.global_avg, 7) for k, meter in metric_logger.meters.items()}\n",
    "    print(\"Averaged train_stats: \", train_stats)\n",
    "    \n",
    "    ################################################################################################\n",
    "    # Validation\n",
    "    ################################################################################################\n",
    "    \n",
    "    model.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \", n=1)\n",
    "    header = 'Valid:'\n",
    "\n",
    "    \n",
    "    for batch_data in metric_logger.log_every(data_loader_valid, print_freq, header):\n",
    "        \n",
    "        input  = batch_data[0].to(device).float()\n",
    "        seg_gt = batch_data[1].to(device).float()\n",
    "\n",
    "        seg_pred = model(input)\n",
    "\n",
    "        loss, loss_detail = criterion(seg_pred=seg_pred, seg_gt=seg_gt)\n",
    "    \n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "\n",
    "        # LOSS\n",
    "        metric_logger.update(loss=loss_value)        \n",
    "        if loss_detail is not None:\n",
    "            metric_logger.update(**loss_detail)\n",
    "\n",
    "        # post-processing\n",
    "        seg_pred = torch.sigmoid(seg_pred)\n",
    "\n",
    "\n",
    "        # Metrics SEG\n",
    "        if seg_gt.any():\n",
    "            dice = soft_dice_score(output=seg_pred.round(), target=seg_gt, smooth=0.0)    # pred_seg must be round() !! \n",
    "            metric_logger.update(dice=dice.item())     \n",
    "    \n",
    "    valid_stats = {k: round(meter.global_avg, 7) for k, meter in metric_logger.meters.items()}\n",
    "    print(\"Averaged valid_stats: \", valid_stats)\n",
    "    \n",
    "    ################################################################################################\n",
    "    # Save & Log\n",
    "    ################################################################################################\n",
    "    \n",
    "    checkpoint_paths = output_dir + '/epoch_' + str(epoch) + '_checkpoint.pth'\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "    }, checkpoint_paths)\n",
    "\n",
    "    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                **{f'valid_{k}': v for k, v in valid_stats.items()},\n",
    "                'epoch': epoch}\n",
    "\n",
    "    if output_dir:\n",
    "        with open(output_dir + \"/log.txt\", \"a\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_log(path):\n",
    "    log_list = []\n",
    "    lines = open(path, 'r').read().splitlines() \n",
    "    for i in range(len(lines)):\n",
    "        exec('log_list.append('+lines[i] + ')')\n",
    "    return  log_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "51, 36, 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_list = read_log(path = '/workspace/sunggu/6.Kakao/checkpoints/[Baseline]Face_Net_DAC_fold0_upsample/log.txt')\n",
    "print(log_list[0].keys())\n",
    "result_dict = {}\n",
    "\n",
    "for key in log_list[0].keys():\n",
    "    exec( \"result_dict['\"+str(key)+\"']\" + \" = [ log_list[i]['\"+str(key)+\"'] for i in range(len(log_list)) ]\")\n",
    "\n",
    "for key in result_dict.keys():\n",
    "    plt.plot(result_dict[key])\n",
    "    plt.title(key)\n",
    "    print(\"###########################################################\")\n",
    "    print(\"Argsort = \", np.argsort(result_dict[key])[:5])\n",
    "    print(\"Value   = \", [result_dict[key][i] for i in np.argsort(result_dict[key])[:5]])\n",
    "    plt.show()\n",
    "    \n",
    "    if key == 'valid_loss':\n",
    "        print(\"Valid_Loss = \", np.argsort(result_dict[key])[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log check\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_log(path):\n",
    "    log_list = []\n",
    "    lines = open(path, 'r').read().splitlines() \n",
    "    for i in range(len(lines)):\n",
    "        exec('log_list.append('+lines[i] + ')')\n",
    "    return  log_list\n",
    "\n",
    "51, 36, 35\n",
    "\n",
    "log_list = read_log(path = '/workspace/sunggu/6.Kakao/checkpoints/[Baseline]Face_Net_DAC_fold0_upsample/log.txt')\n",
    "print(log_list[0].keys())\n",
    "result_dict = {}\n",
    "\n",
    "for key in log_list[0].keys():\n",
    "    exec( \"result_dict['\"+str(key)+\"']\" + \" = [ log_list[i]['\"+str(key)+\"'] for i in range(len(log_list)) ]\")\n",
    "\n",
    "for key in result_dict.keys():\n",
    "    plt.plot(result_dict[key])\n",
    "    plt.title(key)\n",
    "    print(\"###########################################################\")\n",
    "    print(\"Argsort = \", np.argsort(result_dict[key])[:5])\n",
    "    print(\"Value   = \", [result_dict[key][i] for i in np.argsort(result_dict[key])[:5]])\n",
    "    plt.show()\n",
    "    \n",
    "    if key == 'valid_loss':\n",
    "        print(\"Valid_Loss = \", np.argsort(result_dict[key])[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "\n",
    "print(\"Loading... Resume\")\n",
    "checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])        \n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])        \n",
    "args.start_epoch = checkpoint['epoch'] + 1  \n",
    "try:\n",
    "    log_path = os.path.dirname(args.resume)+'/log.txt'\n",
    "    lines    = open(log_path,'r').readlines()\n",
    "    val_loss_list = []\n",
    "    for l in lines:\n",
    "        exec('log_dict='+l.replace('NaN', '0'))\n",
    "        val_loss_list.append(log_dict['valid_loss'])\n",
    "    print(\"Epoch: \", np.argmin(val_loss_list), \" Minimum Val Loss ==> \", np.min(val_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "STL_cls.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
